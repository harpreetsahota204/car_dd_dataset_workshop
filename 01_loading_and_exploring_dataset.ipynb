{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/car_dd_dataset_workshop/blob/main/01_loading_and_exploring_dataset.ipynb)\n",
    "\n",
    "Note: If using in Google Colab, make sure you [install all the requirements listed here](https://github.com/harpreetsahota204/car_dd_dataset_workshop/blob/main/requirements.txt).\n",
    "\n",
    "# Download dataset from source\n",
    "\n",
    "We can download the file from Google Drive using `gdown`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1RgiZK970s0BffZiAkwmxrdJ7klqIY2PZ\n",
      "From (redirected): https://drive.google.com/uc?id=1RgiZK970s0BffZiAkwmxrdJ7klqIY2PZ&confirm=t&uuid=8709d12c-00e2-43d7-814b-7fabbcbf7132\n",
      "To: /home/harpreet/workspace/car_dd_dataset_workshop/cardd.zip\n",
      "  0%|          | 11.0M/6.05G [00:00<03:45, 26.8MB/s]"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "# Download the CarDD dataset from Google Drive\n",
    "url = \"https://drive.google.com/uc?id=1RgiZK970s0BffZiAkwmxrdJ7klqIY2PZ\"\n",
    "gdown.download(url, output=\"cardd.zip\", quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then extract the dataset as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip cardd.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into FiftyOne Format\n",
    "\n",
    "FiftyOne [supports importing datasets from disk in various formats](https://beta-docs.voxel51.com/fiftyone_concepts/dataset_creation/), and it can be extended to import datasets in custom formats. The basic recipe involves specifying the path(s) to the data on disk and the type of dataset you’re loading. \n",
    "\n",
    "You can import a dataset from disk via [the `from_dir()` method](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#from_dir). \n",
    "\n",
    "Read the docs for full detail on all [supported formats](https://beta-docs.voxel51.com/fiftyone_concepts/dataset_creation/datasets/#loading-datasets-from-disk).\n",
    "\n",
    "The CarDD dataset is in COCO format, so you can use [FiftyOne's built-in importer for COCO dataset](https://beta-docs.voxel51.com/fiftyone_concepts/dataset_creation/datasets/#cocodetectiondataset). \n",
    "\n",
    "The relevant arguments we use here are:\n",
    "\n",
    "• `data_path` - where the images reside on disk\n",
    "\n",
    "• `labels_path` - the path to the annotations, which should be a `json` file\n",
    "\n",
    "• `dataset_type` - let' FiftyOne know we are loading a Dataset in COCO format\n",
    "\n",
    "Read [the docs to learn more](https://beta-docs.voxel51.com/api/fiftyone.utils.coco.html#fiftyone.utils.coco.COCODetectionDatasetImporter) about working with datasets in COCO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    data_path=\"CarDD_release/CarDD_COCO/train2017\",\n",
    "    labels_path=\"CarDD_release/CarDD_COCO/annotations/instances_train2017.json\",\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    name=\"car_dd\",\n",
    "    overwrite=True,\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call the dataset to see it's associated fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's [persist the Dataset](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/#dataset-persistence) as non-persistent datasets are deleted from the database each time the database is shut down. Note, you could define dataset persistence when you create the dataset by passing `persistent=True` into the `from_dir` method above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.persistent = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also call [the first Sample of the Dataset](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#first) to see what the Fields looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that bounding box detections and the segmentations are parsed as [FiftyOne Detection types](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html).\n",
    "\n",
    "FiftyOne Detections are relative bounding box coordinates in `[0, 1]` in the following format: `[top-left-x, top-left-y, width, height]`.\n",
    "\n",
    "\n",
    "\n",
    "Since we are working with instance segmentation the labels are parsed via the Detection label type with `mask` defining an instance segmentation mask for the detection within its bounding box. These are parsed as a 2D binary or 0/1 integer `numpy` array.\n",
    "\n",
    "# Alternatively, download dataset from Hugging Face Hub\n",
    "\n",
    "FiftyOne has an [integration with Hugging Face](https://beta-docs.voxel51.com/integrations/huggingface/), which allows you to push and pull FiftyOne Datasets from the Hugging Face Hub.\n",
    "\n",
    "For the purposes of this workshop, I have have already [parsed this dataset into FiftyOne format and pushed it to the Hub](https://huggingface.co/datasets/harpreetsahota/CarDD).\n",
    "\n",
    "You can download it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "hub_dataset = load_from_hub(\n",
    "    \"harpreetsahota/CarDD\",\n",
    "    name=\"cardd_from_hub\",\n",
    "    # max_samples=500, # if you want to work with a subset of the dataset\n",
    "    persistent=True,\n",
    "    overwrite=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your Dataset loaded into FiftyOne format, now is a good time to launch the App and perform a \"visual vibe check\" of its contents. \n",
    "\n",
    "You can launch the app in a notebook by running:\n",
    "\n",
    "```python\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "fo.launch_app(hub_dataset)\n",
    "```\n",
    "\n",
    "Or, you can open your terminal and execute `fiftyone app launch`. This will open the App in a browser window, and you can select your Dataset from the dropdown menu.\n",
    "\n",
    "# Exploration via FiftyOne App\n",
    "\n",
    "High resolution images may take unnecessary time to load. Sometimes it's useful to create thumbnails of the full resolution images and load those in the App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.image as foui\n",
    "\n",
    "THUMBNAIL_SIZE = 224\n",
    "\n",
    "foui.transform_images(\n",
    "        hub_dataset,\n",
    "        size=(-1, THUMBNAIL_SIZE),\n",
    "        output_field=\"thumbnails_path\",\n",
    "        output_dir=\"thumbnails\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll also need to set the following properties in your App config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_dataset.app_config.media_fields = [\"filepath\", \"thumbnails_path\"]\n",
    "hub_dataset.app_config.grid_media_field = \"thumbnails_path\"\n",
    "hub_dataset.save()  # must save after edits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the App in a variety of ways to explore your Dataset. One way to do this is by making using of [the Dashboard plugin](https://github.com/voxel51/fiftyone-plugins/tree/main/plugins/dashboard), which allows you to create interactive dashboards and explore the Dataset in detail.\n",
    "\n",
    "We'll discuss [using and developing plugins](https://beta-docs.voxel51.com/plugins/using_plugins/) later in this workshop, but for now let's go ahead and install the required plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download \\\n",
    "    https://github.com/voxel51/fiftyone-plugins \\\n",
    "    --plugin-names @voxel51/dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial exploration via SDK\n",
    "\n",
    "> If you're a seasoned Pandas user, you might want to learn more about performing Pandas-style queries in FiftyOne. Read [these docs to learn more](https://beta-docs.voxel51.com/tutorials/pandas_comparison/).\n",
    "\n",
    "Let's explore the Dataset via the SDK before launching the FiftyOne App. From here on out, we will make use of the dataset that we have downloaded from the Hugging Face Hub\n",
    "\n",
    "You'll make use of a [`ViewExpression`](https://beta-docs.voxel51.com/api/fiftyone.core.expressions.ViewExpression.html) and [`ViewField`](https://beta-docs.voxel51.com/api/fiftyone.core.expressions.ViewField.html) to perform aggregrations over fields of a FiftyOne dataset. Using `ViewField` allows efficient calculation across the entire dataset without manual iteration.  \n",
    "\n",
    "You can learn more about aggregrations in FiftyOne by [reading these docs](https://beta-docs.voxel51.com/fiftyone_concepts/using_aggregations/) and learn more about creating `Views` in [these docs](https://beta-docs.voxel51.com/how_do_i/recipes/creating_views/).\n",
    "\n",
    "Let's see the counts and types of damages in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "hub_dataset.count_values(F\"detections.detections.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching the dataset\n",
    "\n",
    "You notice that beyond the bounding boxes and segmentation masks, there is not much other information on this dataset. However, you can use Fiftyone to enrich your dataset. \n",
    "\n",
    "Something you might be interested in is the are of the bounding boxes. Let's start by adding this information. The code below will help us compute this value. Here's what is happening:\n",
    "\n",
    "- `rel_bbox_area`: Calculates bounding box area (width * height) as fraction of image size\n",
    "\n",
    "- `im_width, im_height`: Gets image dimensions from metadata\n",
    "\n",
    "- `abs_area`: Converts relative area to pixels by multiplying with image dimensions\n",
    "\n",
    "The code adds two fields to each detection:\n",
    "\n",
    "1. `relative_bbox_area`: Area as fraction of image (0-1). Note: represent the percentage of the total image area.\n",
    "\n",
    "2. `absolute_bbox_area`: Area in pixels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "rel_bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\n",
    "\n",
    "im_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\")\n",
    "\n",
    "abs_area = rel_bbox_area * im_width * im_height\n",
    "\n",
    "hub_dataset.set_field(\"detections.detections.relative_bbox_area\", rel_bbox_area).save()\n",
    "\n",
    "hub_dataset.set_field(\"detections.detections.absolute_bbox_area\", abs_area).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these values computed you can perform some useful aggregations, for example you  can compute the [upper and lower bounds of the bounding box areas](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#bounds) as well as other summary statistics like mean and standard deviation.\n",
    "\n",
    "This code performs analysis on a car damage dataset (CarDD) loaded into FiftyOne. For each damage type (like \"scratch\", \"dent\", \"crack\", etc.), it:\n",
    "\n",
    "1. Creates a filtered view containing only detections with that specific label\n",
    "2. Calculates bounds (min/max) of the bounding box areas as percentage of image size\n",
    "3. Computes the mean area and standard deviation\n",
    "4. Formats and prints a summary showing the distribution of bounding box sizes for each damage type\n",
    "\n",
    "This helps understand the relative size characteristics of different damage types - for instance, whether scratches tend to be smaller or larger than dents, which types have more size variation, and what the extreme values look like across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = hub_dataset.distinct(\"detections.detections.label\")\n",
    "\n",
    "for label in labels:\n",
    "    view = hub_dataset.filter_labels(\"detections\", F(\"label\") == label)\n",
    "    bounds = view.bounds(\"detections.detections.relative_bbox_area\")\n",
    "    bounds = (bounds[0]*100, bounds[1]*100)\n",
    "    area = view.mean(\"detections.detections.relative_bbox_area\")*100\n",
    "    std = view.std(\"detections.detections.relative_bbox_area\")\n",
    "    print(\"\\033[1m%s:\\033[0m Min: %.4f, Mean: %.4f, Std: %.4f, Max: %.4f\" % (label, bounds[0], std, area, bounds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of the CarDD damage dataset, this analysis is useful because:\n",
    "\n",
    "1. It helps understand the physical characteristics of different damage types - scratches might be long and thin, while dents could be more compact\n",
    "\n",
    "2. It identifies potential challenges for damage detection models - very small damages might be harder to detect\n",
    "\n",
    "3. It can inform model architecture decisions - if damage sizes vary dramatically, you might need a model that handles multi-scale features well\n",
    "\n",
    "4. It helps with data filtering and subset creation - you could focus on smaller damages for fine-grained detection tasks\n",
    "\n",
    "5. It provides dataset quality insights - unusually large or small annotations might indicate labeling errors\n",
    "\n",
    "6. It enables more targeted data augmentation - you could apply transformations specifically for damage types with limited size variations\n",
    "\n",
    "This size distribution knowledge ultimately helps build more robust computer vision models for vehicle damage assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also filter to Samples which have a scratch which meets some condition (as defined by their relative bounding box areas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "filter_to_scratch = F(\"label\") == \"scratch\"\n",
    "\n",
    "filter_to__boxes = F(\"relative_bbox_area\") < 0.03\n",
    "\n",
    "filtered_scratches = hub_dataset.match_labels(\n",
    "    filter=(filter_to_scratch & filter_to_scratch), \n",
    "    fields=\"detections.detections\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that we have just created a View into our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(filtered_scratches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save this view to the Dataset so that you can visualize them later in the FiftyOne App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_dataset.save_view(\"filtered_scratches\", filtered_scratches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, for whatever reason, you want to delete a View you can run:\n",
    "\n",
    "```python\n",
    "hub_dataset.delete_saved_view(\"<view-name>\")\n",
    "```\n",
    "\n",
    "Or delete all the saved Views as follows:\n",
    "\n",
    "```python\n",
    "hub_dataset.delete_saved_views()\n",
    "```\n",
    "\n",
    "\n",
    "> Check out [this tutorial for more information](https://github.com/harpreetsahota204/Hands-on-Data-Centric-Visual-AI/blob/main/Module-2/Lesson_1_Exploring_Your_Dataset_with_FiftyOne.ipynb) about doing complex aggregations and filtering in FiftyOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing surface area of damages\n",
    "\n",
    "This code takes a dataset of car damage images with their associated damage annotations and converts their representation into a more useful format. \n",
    "\n",
    "1. First, it extracts all the ground truth detection masks from the dataset using the [`values` method](https://beta-docs.voxel51.com/api/fiftyone.core.aggregations.Values.html) of the Dataset. \n",
    "\n",
    "2. It then prepares to convert these masks into a [FiftyOne Polyline](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Polyline.html), which are essentially outlines of the damaged areas defined by a series of connected points. \n",
    "\n",
    "3. The code then goes through each image's detections one by one:\n",
    "   - For each image that has damage annotations, it converts all the damage masks into a FiftyOne Polyline.\n",
    "   - If an image has no damage annotations, it creates an empty list instead\n",
    "   - It packages these polylines into a [FiftyOne Polylines](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Polylines.html), which is just a a list of Polylines or polygons in an image.\n",
    "\n",
    "4. Finally, it adds all these polyline representations back to the dataset as a new field called `polylines`. \n",
    "\n",
    "This conversion is useful because polylines can be easier to work with for certain types of analysis, like calculating the area of damaged regions or visualizing the boundaries of damage. It's like having both a coloring book (the masks) and just the outlines (the polylines) - each format has its own advantages for different tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all ground truth detection masks from the dataset\n",
    "# This returns a list of Detections objects, one per sample\n",
    "segmentation_masks = hub_dataset.values(\"detections.detections\")\n",
    "\n",
    "# Initialize an empty list to store polyline representations for each sample\n",
    "all_polylines = []\n",
    "\n",
    "# Iterate through detections for each sample in the dataset\n",
    "for sample_segmentation in segmentation_masks:\n",
    "    # For each detection in the sample, convert its segmentation mask to a polyline\n",
    "    # If sample has no detections (None), create empty list\n",
    "    polylines = [segmentation.to_polyline() for segmentation in sample_segmentation] if sample_segmentation else []\n",
    "    \n",
    "    # Create a FiftyOne Polylines field containing the polyline representations\n",
    "    polylines_field = fo.Polylines(\n",
    "        polylines=polylines,\n",
    "        closed=True,\n",
    "        filled=True,\n",
    "        )\n",
    "    \n",
    "    # Add the polylines for this sample to our list\n",
    "    all_polylines.append(polylines_field)\n",
    "\n",
    "# Add the polylines field to every sample in the dataset\n",
    "# This creates a new field called \"polylines\" containing the polyline representations\n",
    "hub_dataset.set_values(\"polylines\", all_polylines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a function which will compute the area of a polygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_polygon_area(points, image_width, image_height):\n",
    "    \"\"\"\n",
    "    Compute the area of a polygon in pixel units using the Shoelace formula.\n",
    "    \n",
    "    Args:\n",
    "        points: List of (x,y) coordinates defining the polygon vertices, normalized to [0,1]\n",
    "        image_width: Width of the image in pixels\n",
    "        image_height: Height of the image in pixels\n",
    "        \n",
    "    Returns:\n",
    "        float: Area of the polygon in square pixels\n",
    "        \n",
    "    Notes:\n",
    "        The Shoelace formula (also known as the surveyor's formula) calculates the area \n",
    "        of a polygon by using the coordinates of its vertices. The formula gets its name\n",
    "        from the way the computation \"laces\" together vertex coordinates.\n",
    "    \"\"\"\n",
    "    # Convert points list to numpy array for vectorized operations\n",
    "    points = np.array(points)\n",
    "    \n",
    "    # Scale normalized coordinates back to pixel dimensions\n",
    "    points[:, 0] *= image_width  # Scale x coordinates\n",
    "    points[:, 1] *= image_height # Scale y coordinates\n",
    "    \n",
    "    # Extract x and y coordinates into separate arrays\n",
    "    x = points[:, 0]\n",
    "    y = points[:, 1]\n",
    "    \n",
    "    # Create shifted versions of coordinate arrays\n",
    "    # np.roll shifts array elements by 1 position for the formula\n",
    "    x_shift = np.roll(x, 1)\n",
    "    y_shift = np.roll(y, 1)\n",
    "    \n",
    "    # Apply Shoelace formula: A = 1/2 * |sum(x_i*y_i+1 - x_i+1*y_i)|\n",
    "    return 0.5 * np.abs(np.sum(x * y_shift - x_shift * y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can add in the absolute and relative surface areas.\n",
    "\n",
    "This code processes each sample in a car damage dataset to calculate the surface area of the damage regions. For each sample:\n",
    "\n",
    "1. It extracts the polygon coordinates (`points`) from the first polyline in the sample. These polylines represent the outlines of damaged areas on the car.\n",
    "\n",
    "2. It retrieves the image dimensions (`width` and `height`) from the sample's metadata.\n",
    "\n",
    "3. It calculates the `absolute_surface_area` of the damage in pixels using a helper function called `compute_polygon_area`, which likely uses the Shoelace formula to calculate the area of an irregular polygon.\n",
    "\n",
    "4. It calculates the `relative_surface_area` as a proportion of the total image area, but there's a bug: it uses an undefined variable `area` instead of `absolute_surface_area`.\n",
    "\n",
    "5. It adds both measurements as properties to the polyline object.\n",
    "\n",
    "6. It saves the updated sample back to the dataset.\n",
    "\n",
    "This information is valuable for analyzing damage severity and comparing different types of damage across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for sample in hub_dataset:\n",
    "    # Get the points - take the first list from the nested structure\n",
    "    points = np.array(sample.polylines.polylines[0].points[0])  # Note the [0] to get first list\n",
    "    \n",
    "    # Get image dimensions\n",
    "    width = sample.metadata.width\n",
    "    height = sample.metadata.height\n",
    "    \n",
    "    # Compute area using the helper function\n",
    "    absolute_surface_area = compute_polygon_area(points, width, height)\n",
    "\n",
    "    relative_surface_area = area / (width * height)\n",
    "    \n",
    "    # Store both relative and absolute areas\n",
    "    sample.polylines.polylines[0].relative_surface_area = relative_surface_area\n",
    "    sample.polylines.polylines[0].absolute_surface_area = absolute_surface_area\n",
    "    \n",
    "    # Save the sample\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = hub_dataset.distinct(\"polylines.polylines.label\")\n",
    "\n",
    "for label in labels:\n",
    "    view = hub_dataset.filter_labels(\"polylines\", F(\"label\") == label)\n",
    "    bounds = view.bounds(\"polylines.polylines.relative_bbox_area\")\n",
    "    bounds = (bounds[0]*100, bounds[1]*100)\n",
    "    area = view.mean(\"polylines.polylines.relative_bbox_area\")*100\n",
    "    std = view.std(\"polylines.polylines.relative_bbox_area\")\n",
    "    print(\"\\033[1m%s:\\033[0m Min: %.4f, Mean: %.4f, Std: %.4f, Max: %.4f\" % (label, bounds[0], std, area, bounds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VLMs for data enrichment\n",
    "\n",
    "You can use Vision Language Models (VLMs) to enrich car damage analysis in some interesting ways:\n",
    "\n",
    "1. **Basic Captioning:** Generate descriptions of damage type, severity, and affected car parts in natural language.\n",
    "\n",
    "2. **Detailed Location:** Convert basic annotations (like \"dent on door\") into precise descriptions (like \"dent on lower passenger door near handle\").\n",
    "\n",
    "3. **Damage Cause Analysis:** Infer potential causes from visual clues (e.g., \"scratch likely from brushing against object\").\n",
    "\n",
    "4. **Scene Context:** Describe relevant environmental factors (e.g., \"parked car, daylight conditions, clean vehicle\").\n",
    "\n",
    "5. **Multi-Damage Relations:** Explain how multiple damages relate (e.g., \"scratch running across dent\" or \"cluster of dents\").\n",
    "\n",
    "6. **Component Recognition:** Identify and label both damaged and undamaged car parts for better context.\n",
    "\n",
    "\n",
    "FiftyOne allows you to use VLMs rather seemlessly, for example [Qwen2.5-VL is a VLM](https://github.com/harpreetsahota204/qwen2_5_vl) which has been integrated as a [Remotely Sourced Zoo Model](https://beta-docs.voxel51.com/models/model_zoo/remote/) (which we will discuss later).\n",
    "\n",
    "Start by registering the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/qwen2_5_vl\",\n",
    "    overwrite=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/qwen2_5_vl\",\n",
    "    model_name=\"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can download a checkpoint. Refer to the [Qwen2.5-VL's Remote Zoo Model's GitHub](https://github.com/harpreetsahota204/qwen2_5_vl/tree/main) to see the available checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can load the model as you would any [Built-in Zoo Model](https://beta-docs.voxel51.com/models/model_zoo/models/).\n",
    "\n",
    "Ttart by setting `operation=\"vqa\"`, which we will use to generate answers/captions for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz \n",
    "\n",
    "zoo_model = foz.load_zoo_model(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    # install_requirements=True #if you are using for the first time and need to download reuirement,\n",
    "    # ensure_requirements=True #  ensure any requirements are installed before loading the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set the prompt for the model by as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo_model.operation=\"vqa\"\n",
    "zoo_model.prompt=\"Complete a damage report for this vehicle in this image. Include details about the damage, including the location and type of damage. If there is no damage, say 'No damage'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then apply the model to your Dataset via [the `apply_model` method](https://beta-docs.voxel51.com/api/fiftyone.core.models.html#apply_model) of the Dataset.\n",
    "\n",
    "Image captioning/VQA typically takes longer other operations. This took ~17 minutes to run on a single NVIDIA RTX 600 Ada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_dataset.apply_model(zoo_model, label_field=\"damage_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the model for classification, for example to get the color of the vehicle. All you have to do is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo_model.operation=\"classify\"\n",
    "zoo_model.prompt = \"What is the color of the damaged vehicle in this image? Please provide the color name.\"\n",
    "hub_dataset.apply_model(zoo_model, label_field=\"vehicle_color\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also classify the location of the damage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo_model.prompt = \"\"\"You are required to report the location of vehicle damage. \\\n",
    "    List all the locations where this vehicle has been damaged. Choose from one or more of the following, or include any location not explicitly listed: \n",
    "    - quarter panel\n",
    "    - driver door\n",
    "    - passenger door\n",
    "    - rear door\n",
    "    - hood\n",
    "    - front bumper\n",
    "    - rear bumper\n",
    "    - quarter panel\n",
    "    - tires\n",
    "    - rim \n",
    "    - wheel well\n",
    "    - window\n",
    "    - windshield     \n",
    "    \n",
    "    Where all the locations where this vehicle has been damaged?\"\"\"\n",
    "\n",
    "hub_dataset.apply_model(zoo_model, label_field=\"damage_location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly explore what the VLM has come up with. For context, let's open the first image and examine the model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.open(hub_dataset.first().filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hub_dataset.first().damage_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_dataset.first().vehicle_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_dataset.first().damage_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE BEFORE PUSH\n",
    "hub_dataset.save()\n",
    "hub_dataset.persistent = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, at this point it will be helpful for us to install the [Caption Viewer Plugin](https://github.com/mythrandire/caption-viewer) so that we can more easily read the captions as we explore them in the App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/mythrandire/caption-viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(hub_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also refresh the FiftyOne App and view the model output for each sample in the Dataset.\n",
    "\n",
    "## Your turn now!\n",
    "\n",
    "There are several VLMs which are integrated as remotely sourced zoo models.\n",
    "\n",
    "Pick from one (or more) of the following and see what you can get these models to do. Each of these repos has a complete notebook example and thorough README:\n",
    "\n",
    "- [Moondream2](https://github.com/harpreetsahota204/moondream2)\n",
    "\n",
    "- [Florence2](https://github.com/harpreetsahota204/florence2)\n",
    "\n",
    "- [PaliGemma2](https://github.com/harpreetsahota204/paligemma2)\n",
    "\n",
    "By the way, you may want to push your dataset to Hugging Face so that you can access it later. \n",
    "\n",
    "Make sure you have a Hugging Face token, and then sign in via the CLI: `huggingface-cli login` and enter your token.\n",
    "\n",
    "Once you've done that, you can push your dataset to the hub as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "from fiftyone.utils.huggingface import push_to_hub\n",
    "\n",
    "push_to_hub(\n",
    "    dataset,\n",
    "    \"<some-repo-name>\"\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
