{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Download dataset from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1bbyqVCKZX5Ur5Zg-uKj0jD0maWAVeOLx\n",
      "From (redirected): https://drive.google.com/uc?id=1bbyqVCKZX5Ur5Zg-uKj0jD0maWAVeOLx&confirm=t&uuid=56312caa-0a4f-4a1e-a6b6-e66f96bb95cc\n",
      "To: /Users/harpreetsahota/workspace/car_dd_dataset_workshop/cardd_dataset.zip\n",
      "100%|██████████| 6.05G/6.05G [06:22<00:00, 15.8MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cardd_dataset.zip'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "# Download the CarDD dataset from Google Drive\n",
    "url = \"https://drive.google.com/uc?id=1bbyqVCKZX5Ur5Zg-uKj0jD0maWAVeOLx\"\n",
    "gdown.download(url, output=\"cardd_dataset.zip\", quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then extract the dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip cardd_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into FiftyOne Format\n",
    "\n",
    "FiftyOne [supports importing datasets from disk in various formats](https://beta-docs.voxel51.com/fiftyone_concepts/dataset_creation/), and it can be extended to import datasets in custom formats. The basic recipe involves specifying the path(s) to the data on disk and the type of dataset you’re loading. \n",
    "\n",
    "You can import a dataset from disk via [the `from_dir()` method](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#from_dir). \n",
    "\n",
    "Read the docs for full detail on all [supported formats](https://beta-docs.voxel51.com/fiftyone_concepts/dataset_creation/datasets/#loading-datasets-from-disk).\n",
    "\n",
    "The CarDD dataset is in COCO format, so you can use [FiftyOne's built-in importer for COCO dataset](https://beta-docs.voxel51.com/fiftyone_concepts/dataset_creation/datasets/#cocodetectiondataset). \n",
    "\n",
    "The relevant arguments we use here are:\n",
    "\n",
    "• `data_path` - where the images reside on disk\n",
    "\n",
    "• `labels_path` - the path to the annotations, which should be a `json` file\n",
    "\n",
    "• `dataset_type` - let' FiftyOne know we are loading a Dataset in COCO format\n",
    "\n",
    "Read [the docs to learn more](https://beta-docs.voxel51.com/api/fiftyone.utils.coco.html#fiftyone.utils.coco.COCODetectionDatasetImporter) about working with datasets in COCO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 2816/2816 [12.3s elapsed, 0s remaining, 220.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    data_path=\"CarDD_release/CarDD_COCO/train2017\",\n",
    "    labels_path=\"CarDD_release/CarDD_COCO/annotations/instances_train2017.json\",\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    name=\"car_dd\",\n",
    "    overwrite=True,\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call the dataset to see it's associated fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        car_dd\n",
       "Media type:  image\n",
       "Num samples: 2816\n",
       "Persistent:  False\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:               fiftyone.core.fields.ObjectIdField\n",
       "    filepath:         fiftyone.core.fields.StringField\n",
       "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:       fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
       "    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    segmentations:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    coco_id:          fiftyone.core.fields.IntField"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's [persist the Dataset](https://beta-docs.voxel51.com/fiftyone_concepts/using_datasets/#dataset-persistence) as non-persistent datasets are deleted from the database each time the database is shut down. Note, you could define dataset persistence when you create the dataset by passing `persistent=True` into the `from_dir` method above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.persistent = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also call [the first Sample of the Dataset](https://beta-docs.voxel51.com/api/fiftyone.core.dataset.Dataset.html#first) to see what the Fields looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sample: {\n",
       "    'id': '67f528008be56ae5152c0ecb',\n",
       "    'media_type': 'image',\n",
       "    'filepath': '/Users/harpreetsahota/workspace/car_dd_dataset_workshop/CarDD_release/CarDD_COCO/train2017/000001.jpg',\n",
       "    'tags': [],\n",
       "    'metadata': <ImageMetadata: {\n",
       "        'size_bytes': None,\n",
       "        'mime_type': None,\n",
       "        'width': 1000,\n",
       "        'height': 750,\n",
       "        'num_channels': None,\n",
       "    }>,\n",
       "    'created_at': datetime.datetime(2025, 4, 8, 13, 43, 28, 988000),\n",
       "    'last_modified_at': datetime.datetime(2025, 4, 8, 13, 43, 28, 988000),\n",
       "    'detections': <Detections: {\n",
       "        'detections': [\n",
       "            <Detection: {\n",
       "                'id': '67f528008be56ae5152c0ec7',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'scratch',\n",
       "                'bounding_box': [0.16704, 0.05361333333333333, 0.20279, 0.17512],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'iscrowd': 0,\n",
       "                'occluded': False,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67f528008be56ae5152c0ec8',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'tire flat',\n",
       "                'bounding_box': [\n",
       "                    0.16066,\n",
       "                    0.14993333333333334,\n",
       "                    0.6841900000000001,\n",
       "                    0.7346933333333333,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'mask_path': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'iscrowd': 0,\n",
       "                'occluded': False,\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "    'segmentations': <Detections: {\n",
       "        'detections': [\n",
       "            <Detection: {\n",
       "                'id': '67f528008be56ae5152c0ec9',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'scratch',\n",
       "                'bounding_box': [0.16704, 0.05361333333333333, 0.20279, 0.17512],\n",
       "                'mask': array([[False, False, False, ..., False, False, False],\n",
       "                       [False, False, False, ..., False, False, False],\n",
       "                       [False, False, False, ..., False, False, False],\n",
       "                       ...,\n",
       "                       [False, False, False, ..., False, False, False],\n",
       "                       [False, False, False, ..., False, False, False],\n",
       "                       [False, False, False, ..., False, False, False]], shape=(132, 203)),\n",
       "                'mask_path': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'iscrowd': 0,\n",
       "                'occluded': False,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '67f528008be56ae5152c0eca',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'tire flat',\n",
       "                'bounding_box': [\n",
       "                    0.16066,\n",
       "                    0.14993333333333334,\n",
       "                    0.6841900000000001,\n",
       "                    0.7346933333333333,\n",
       "                ],\n",
       "                'mask': array([[False, False, False, ..., False, False, False],\n",
       "                       [False, False, False, ..., False, False, False],\n",
       "                       [False, False, False, ..., False, False, False],\n",
       "                       ...,\n",
       "                       [False, False, False, ..., False, False, False],\n",
       "                       [False, False, False, ..., False, False, False],\n",
       "                       [False, False, False, ..., False, False, False]], shape=(551, 684)),\n",
       "                'mask_path': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'iscrowd': 0,\n",
       "                'occluded': False,\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "    'coco_id': 1,\n",
       "}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that bounding box detections and the segmentations are parsed as [FiftyOne Detection types](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Detection.html).\n",
    "\n",
    "FiftyOne Detections are relative bounding box coordinates in `[0, 1]` in the following format: `[top-left-x, top-left-y, width, height]`.\n",
    "\n",
    "\n",
    "\n",
    "Since we are working with instance segmentation the labels are parsed via the Detection label type with `mask` defining an instance segmentation mask for the detection within its bounding box. These are parsed as a 2D binary or 0/1 integer `numpy` array.\n",
    "\n",
    "# Alternatively, download dataset from Hugging Face Hub\n",
    "\n",
    "FiftyOne has an [integration with Hugging Face](https://beta-docs.voxel51.com/integrations/huggingface/), which allows you to push and pull FiftyOne Datasets from the Hugging Face Hub.\n",
    "\n",
    "Prerna has already [parsed this dataset into FiftyOne format and pushed it to the Hub](https://huggingface.co/datasets/prernadh/car-defect-detection). You can download it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config file fiftyone.yml from prernadh/car-defect-detection\n",
      "Loading dataset\n",
      "Importing samples...\n",
      " 100% |█████████████████| 374/374 [15.8ms elapsed, 0s remaining, 23.7K samples/s]      \n",
      "Downloading 374 media files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:30<00:00,  7.70s/it]\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "hub_dataset = load_from_hub(\n",
    "    \"prernadh/car-defect-detection\",\n",
    "    name=\"cardd_from_hub\",\n",
    "    persistent=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your Dataset loaded into FiftyOne format, now is a good time to launch the App and perform a \"visual vibe check\" of its contents. \n",
    "\n",
    "You can launch the app in a notebook by running:\n",
    "\n",
    "```python\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "fo.launch_app(hub_dataset)\n",
    "```\n",
    "\n",
    "Or, you can open your terminal and execute `fiftyone app launch`. This will open the App in a browser window, and you can select your Dataset from the dropdown menu.\n",
    "\n",
    "# Exploration via FiftyOne App\n",
    "\n",
    "High resolution images may take unnecessary time to load. Sometimes it's useful to create thumbnails of the full resolution images and load those in the App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 374/374 [2.2s elapsed, 0s remaining, 172.7 samples/s]     \n"
     ]
    }
   ],
   "source": [
    "import fiftyone.utils.image as foui\n",
    "\n",
    "THUMBNAIL_SIZE = 224\n",
    "\n",
    "foui.transform_images(\n",
    "        hub_dataset,\n",
    "        size=(-1, THUMBNAIL_SIZE),\n",
    "        output_field=\"thumbnails_path\",\n",
    "        output_dir=\"thumbnails\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll also need to set the following properties in your App config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_dataset.app_config.media_fields = [\"filepath\", \"thumbnails_path\"]\n",
    "hub_dataset.app_config.grid_media_field = \"thumbnails_path\"\n",
    "hub_dataset.save()  # must save after edits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the App in a variety of ways to explore your Dataset. One way to do this is by making using of [the Dashboard plugin](https://github.com/voxel51/fiftyone-plugins/tree/main/plugins/dashboard), which allows you to create interactive dashboards and explore the Dataset in detail.\n",
    "\n",
    "We'll discuss [using and developing plugins](https://beta-docs.voxel51.com/plugins/using_plugins/) later in this workshop, but for now let's go ahead and install the required plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading voxel51/fiftyone-plugins...\n",
      "  329.3Mb [3.4s elapsed, ? remaining, 90.9Mb/s]    \n",
      "Skipping existing plugin '@voxel51/dashboard'\n"
     ]
    }
   ],
   "source": [
    "!fiftyone plugins download \\\n",
    "    https://github.com/voxel51/fiftyone-plugins \\\n",
    "    --plugin-names @voxel51/dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial exploration via SDK\n",
    "\n",
    "> If you're a seasoned Pandas user, you might want to learn more about performing Pandas-style queries in FiftyOne. Read [these docs to learn more](https://beta-docs.voxel51.com/tutorials/pandas_comparison/).\n",
    "\n",
    "Let's explore the Dataset via the SDK before launching the FiftyOne App. From here on out, we will make use of the dataset that we have downloaded from the Hugging Face Hub\n",
    "\n",
    "You'll make use of a [`ViewExpression`](https://beta-docs.voxel51.com/api/fiftyone.core.expressions.ViewExpression.html) and [`ViewField`](https://beta-docs.voxel51.com/api/fiftyone.core.expressions.ViewField.html) to perform aggregrations over fields of a FiftyOne dataset. Using `ViewField` allows efficient calculation across the entire dataset without manual iteration.  \n",
    "\n",
    "You can learn more about aggregrations in FiftyOne by [reading these docs](https://beta-docs.voxel51.com/fiftyone_concepts/using_aggregations/) and learn more about creating `Views` in [these docs](https://beta-docs.voxel51.com/how_do_i/recipes/creating_views/).\n",
    "\n",
    "Let's see the counts and types of damages in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dent': 236,\n",
       " 'lamp broken': 69,\n",
       " 'tire flat': 32,\n",
       " 'scratch': 307,\n",
       " 'crack': 70,\n",
       " 'glass shatter': 71}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "hub_dataset.count_values(F\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching the dataset\n",
    "\n",
    "You notice that beyond the bounding boxes and segmentation masks, there is not much other information on this dataset. However, you can use Fiftyone to enrich your dataset. \n",
    "\n",
    "Something you might be interested in is the are of the bounding boxes. Let's start by adding this information. The code below will help us compute this value. Here's what is happening:\n",
    "\n",
    "- `rel_bbox_area`: Calculates bounding box area (width * height) as fraction of image size\n",
    "\n",
    "- `im_width, im_height`: Gets image dimensions from metadata\n",
    "\n",
    "- `abs_area`: Converts relative area to pixels by multiplying with image dimensions\n",
    "\n",
    "The code adds two fields to each detection:\n",
    "\n",
    "1. `relative_bbox_area`: Area as fraction of image (0-1). Note: represent the percentage of the total image area.\n",
    "\n",
    "2. `absolute_bbox_area`: Area in pixels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot infer an appropriate field type for value '{\n    '$arrayElemAt': [\n        {\n            '$arrayElemAt': [\n                {\n                    '$multiply': [\n                        {'$arrayElemAt': ['$bounding_box', 2]},\n                        {'$arrayElemAt': ['$bounding_box', 3]},\n                    ],\n                },\n                0,\n            ],\n        },\n        0,\n    ],\n}'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m im_width, im_height \u001b[38;5;241m=\u001b[39m F(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$metadata.width\u001b[39m\u001b[38;5;124m\"\u001b[39m), F(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$metadata.height\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m abs_area \u001b[38;5;241m=\u001b[39m rel_bbox_area \u001b[38;5;241m*\u001b[39m im_width \u001b[38;5;241m*\u001b[39m im_height\n\u001b[0;32m----> 9\u001b[0m \u001b[43mhub_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_values\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mground_truth.detections.relative_bbox_area\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_bbox_area\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     11\u001b[0m hub_dataset\u001b[38;5;241m.\u001b[39mset_field(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth.detections.absolute_bbox_area\u001b[39m\u001b[38;5;124m\"\u001b[39m, abs_area)\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/fiftyone/core/collections.py:2439\u001b[0m, in \u001b[0;36mSampleCollection.set_values\u001b[0;34m(self, field_name, values, key_field, skip_none, expand_schema, dynamic, validate, progress, _allow_missing, _sample_ids, _frame_ids)\u001b[0m\n\u001b[1;32m   2285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_values\u001b[39m(\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2287\u001b[0m     field_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2297\u001b[0m     _frame_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2298\u001b[0m ):\n\u001b[1;32m   2299\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the field or embedded field on each sample or frame in the\u001b[39;00m\n\u001b[1;32m   2300\u001b[0m \u001b[38;5;124;03m    collection to the given values.\u001b[39;00m\n\u001b[1;32m   2301\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2437\u001b[0m \u001b[38;5;124;03m            (None), or a progress callback function to invoke instead\u001b[39;00m\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2439\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_none\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2448\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_allow_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_allow_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2449\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_sample_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_sample_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2450\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_frame_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_frame_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/fiftyone/core/collections.py:2489\u001b[0m, in \u001b[0;36mSampleCollection._set_values\u001b[0;34m(self, field_name, values, key_field, skip_none, expand_schema, dynamic, validate, progress, _allow_missing, _sample_ids, _frame_ids)\u001b[0m\n\u001b[1;32m   2484\u001b[0m     _frame_ids, values \u001b[38;5;241m=\u001b[39m _parse_frame_values_dicts(\n\u001b[1;32m   2485\u001b[0m         \u001b[38;5;28mself\u001b[39m, _sample_ids, values\n\u001b[1;32m   2486\u001b[0m     )\n\u001b[1;32m   2488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expand_schema:\n\u001b[0;32m-> 2489\u001b[0m     field, new_group_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_expand_schema_from_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_allow_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2496\u001b[0m     field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/fiftyone/core/collections.py:2875\u001b[0m, in \u001b[0;36mSampleCollection._expand_schema_from_values\u001b[0;34m(self, field_name, values, dynamic, allow_missing, flat)\u001b[0m\n\u001b[1;32m   2869\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset\u001b[38;5;241m.\u001b[39m_add_implied_sample_field(\n\u001b[1;32m   2870\u001b[0m             field_name, value, dynamic\u001b[38;5;241m=\u001b[39mdynamic\n\u001b[1;32m   2871\u001b[0m         )\n\u001b[1;32m   2872\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2873\u001b[0m         \u001b[38;5;66;03m# User didn't request new dynamic attributes to be declared,\u001b[39;00m\n\u001b[1;32m   2874\u001b[0m         \u001b[38;5;66;03m# but we still need to serialize the provided values\u001b[39;00m\n\u001b[0;32m-> 2875\u001b[0m         field \u001b[38;5;241m=\u001b[39m \u001b[43mfoo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_implied_field\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic\u001b[49m\n\u001b[1;32m   2877\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m field, new_group_field\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/fiftyone/core/odm/utils.py:302\u001b[0m, in \u001b[0;36mcreate_implied_field\u001b[0;34m(path, value, dynamic)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates the field for the given value.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m    a :class:`fiftyone.core.fields.Field`\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    301\u001b[0m field_name \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 302\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mget_implied_field_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m create_field(field_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.11/site-packages/fiftyone/core/odm/utils.py:434\u001b[0m, in \u001b[0;36mget_implied_field_kwargs\u001b[0;34m(value, dynamic)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mftype\u001b[39m\u001b[38;5;124m\"\u001b[39m: fof\u001b[38;5;241m.\u001b[39mDictField}\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot infer an appropriate field type for value \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m value\n\u001b[1;32m    436\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot infer an appropriate field type for value '{\n    '$arrayElemAt': [\n        {\n            '$arrayElemAt': [\n                {\n                    '$multiply': [\n                        {'$arrayElemAt': ['$bounding_box', 2]},\n                        {'$arrayElemAt': ['$bounding_box', 3]},\n                    ],\n                },\n                0,\n            ],\n        },\n        0,\n    ],\n}'"
     ]
    }
   ],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "rel_bbox_area = F(\"bounding_box\")[2] * F(\"bounding_box\")[3]\n",
    "\n",
    "im_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\")\n",
    "\n",
    "abs_area = rel_bbox_area * im_width * im_height\n",
    "\n",
    "hub_dataset.set_field(\"ground_truth.detections.relative_bbox_area\", rel_bbox_area).save()\n",
    "\n",
    "hub_dataset.set_field(\"ground_truth.detections.absolute_bbox_area\", abs_area).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these values computed you can perform some useful aggregations, for example you  can compute the [upper and lower bounds of the bounding box areas](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#bounds) as well as other summary statistics like mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = hub_dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "for label in labels:\n",
    "    view = hub_dataset.filter_labels(\"ground_truth\", F(\"label\") == label)\n",
    "    bounds = view.bounds(\"ground_truth.detections.relative_bbox_area\")\n",
    "    bounds = (bounds[0]*100, bounds[1]*100)\n",
    "    area = view.mean(\"ground_truth.detections.relative_bbox_area\")*100\n",
    "    std = view.std(\"ground_truth.detections.relative_bbox_area\")\n",
    "    print(\"\\033[1m%s:\\033[0m Min: %.4f, Mean: %.4f, Std: %.4f, Max: %.4f\" % (label, bounds[0], std, area, bounds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also filter to Samples which have a scratch which meets some condition (as defined by their relative bounding box areas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "filter_to_scratch = F(\"label\") == \"scratch\"\n",
    "\n",
    "filter_to__boxes = F(\"relative_bbox_area\") < 0.03\n",
    "\n",
    "filtered_scratches = hub_dataset.match_labels(\n",
    "    filter=(filter_to_scratch & filter_to_scratch), \n",
    "    fields=\"ground_truth.detections\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that we have just created a View into our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(filtered_scratches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save this view to the Dataset so that you can visualize them later in the FiftyOne App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_dataset.save_view(\"filtered_scratches\", filtered_scratches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, for whatever reason, you want to delete a View you can run:\n",
    "\n",
    "```python\n",
    "hub_dataset.delete_saved_view(\"<view-name>\")\n",
    "```\n",
    "\n",
    "Or delete all the saved Views as follows:\n",
    "\n",
    "```python\n",
    "hub_dataset.delete_saved_views()\n",
    "```\n",
    "\n",
    "\n",
    "> Check out [this tutorial for more information](https://github.com/harpreetsahota204/Hands-on-Data-Centric-Visual-AI/blob/main/Module-2/Lesson_1_Exploring_Your_Dataset_with_FiftyOne.ipynb) about doing complex aggregations and filtering in FiftyOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing surface area of damages\n",
    "\n",
    "This code takes a dataset of car damage images with their associated damage annotations and converts their representation into a more useful format. \n",
    "\n",
    "1. First, it extracts all the ground truth detection masks from the dataset using the [`values` method](https://beta-docs.voxel51.com/api/fiftyone.core.aggregations.Values.html) of the Dataset. \n",
    "\n",
    "2. It then prepares to convert these masks into a [FiftyOne Polyline](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Polyline.html), which are essentially outlines of the damaged areas defined by a series of connected points. \n",
    "\n",
    "3. The code then goes through each image's detections one by one:\n",
    "   - For each image that has damage annotations, it converts all the damage masks into a FiftyOne Polyline.\n",
    "   - If an image has no damage annotations, it creates an empty list instead\n",
    "   - It packages these polylines into a [FiftyOne Polylines](https://beta-docs.voxel51.com/api/fiftyone.core.labels.Polylines.html), which is just a a list of Polylines or polygons in an image.\n",
    "\n",
    "4. Finally, it adds all these polyline representations back to the dataset as a new field called `polylines`. \n",
    "\n",
    "This conversion is useful because polylines can be easier to work with for certain types of analysis, like calculating the area of damaged regions or visualizing the boundaries of damage. It's like having both a coloring book (the masks) and just the outlines (the polylines) - each format has its own advantages for different tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all ground truth detection masks from the dataset\n",
    "# This returns a list of Detections objects, one per sample\n",
    "segmentation_masks = hub_dataset.values(\"ground_truth.detections\")\n",
    "\n",
    "# Initialize an empty list to store polyline representations for each sample\n",
    "all_polylines = []\n",
    "\n",
    "# Iterate through detections for each sample in the dataset\n",
    "for sample_segmentation in segmentation_masks:\n",
    "    # For each detection in the sample, convert its segmentation mask to a polyline\n",
    "    # If sample has no detections (None), create empty list\n",
    "    polylines = [segmentation.to_polyline() for segmentation in sample_segmentation] if sample_segmentation else []\n",
    "    \n",
    "    # Create a FiftyOne Polylines field containing the polyline representations\n",
    "    polylines_field = fo.Polylines(\n",
    "        polylines=polylines,\n",
    "        closed=True,\n",
    "        filled=True,\n",
    "        )\n",
    "    \n",
    "    # Add the polylines for this sample to our list\n",
    "    all_polylines.append(polylines_field)\n",
    "\n",
    "# Add the polylines field to every sample in the dataset\n",
    "# This creates a new field called \"polylines\" containing the polyline representations\n",
    "hub_dataset.set_values(\"polylines\", all_polylines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a function which will compute the area of a polygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_polygon_area(points, image_width, image_height):\n",
    "    \"\"\"\n",
    "    Compute the area of a polygon in pixel units using the Shoelace formula.\n",
    "    \n",
    "    Args:\n",
    "        points: List of (x,y) coordinates defining the polygon vertices, normalized to [0,1]\n",
    "        image_width: Width of the image in pixels\n",
    "        image_height: Height of the image in pixels\n",
    "        \n",
    "    Returns:\n",
    "        float: Area of the polygon in square pixels\n",
    "        \n",
    "    Notes:\n",
    "        The Shoelace formula (also known as the surveyor's formula) calculates the area \n",
    "        of a polygon by using the coordinates of its vertices. The formula gets its name\n",
    "        from the way the computation \"laces\" together vertex coordinates.\n",
    "    \"\"\"\n",
    "    # Convert points list to numpy array for vectorized operations\n",
    "    points = np.array(points)\n",
    "    \n",
    "    # Scale normalized coordinates back to pixel dimensions\n",
    "    points[:, 0] *= image_width  # Scale x coordinates\n",
    "    points[:, 1] *= image_height # Scale y coordinates\n",
    "    \n",
    "    # Extract x and y coordinates into separate arrays\n",
    "    x = points[:, 0]\n",
    "    y = points[:, 1]\n",
    "    \n",
    "    # Create shifted versions of coordinate arrays\n",
    "    # np.roll shifts array elements by 1 position for the formula\n",
    "    x_shift = np.roll(x, 1)\n",
    "    y_shift = np.roll(y, 1)\n",
    "    \n",
    "    # Apply Shoelace formula: A = 1/2 * |sum(x_i*y_i+1 - x_i+1*y_i)|\n",
    "    return 0.5 * np.abs(np.sum(x * y_shift - x_shift * y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can add in the absolute and relative surface areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for sample in hub_dataset:\n",
    "    # Get the points - take the first list from the nested structure\n",
    "    points = np.array(sample.polylines.polylines[0].points[0])  # Note the [0] to get first list\n",
    "    \n",
    "    # Get image dimensions\n",
    "    width = sample.metadata.width\n",
    "    height = sample.metadata.height\n",
    "    \n",
    "    # Compute area using the helper function\n",
    "    absolute_surface_area = compute_polygon_area(points, width, height)\n",
    "\n",
    "    relative_surface_area = area / (width * height)\n",
    "    \n",
    "    # Store both relative and absolute areas\n",
    "    sample.polylines.polylines[0].relative_surface_area = relative_surface_area\n",
    "    sample.polylines.polylines[0].absolute_surface_area = absolute_surface_area\n",
    "    \n",
    "    # Save the sample\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = hub_dataset.distinct(\"polylines.polylines.label\")\n",
    "\n",
    "for label in labels:\n",
    "    view = hub_dataset.filter_labels(\"polylines\", F(\"label\") == label)\n",
    "    bounds = view.bounds(\"polylines.polylines.relative_bbox_area\")\n",
    "    bounds = (bounds[0]*100, bounds[1]*100)\n",
    "    area = view.mean(\"polylines.polylines.relative_bbox_area\")*100\n",
    "    std = view.std(\"polylines.polylines.relative_bbox_area\")\n",
    "    print(\"\\033[1m%s:\\033[0m Min: %.4f, Mean: %.4f, Std: %.4f, Max: %.4f\" % (label, bounds[0], std, area, bounds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VLMs for data enrichment\n",
    "\n",
    "You can use Vision Language Models (VLMs) to enrich car damage analysis in some interesting ways:\n",
    "\n",
    "1. **Basic Captioning:** Generate descriptions of damage type, severity, and affected car parts in natural language.\n",
    "\n",
    "2. **Detailed Location:** Convert basic annotations (like \"dent on door\") into precise descriptions (like \"dent on lower passenger door near handle\").\n",
    "\n",
    "3. **Damage Cause Analysis:** Infer potential causes from visual clues (e.g., \"scratch likely from brushing against object\").\n",
    "\n",
    "4. **Scene Context:** Describe relevant environmental factors (e.g., \"parked car, daylight conditions, clean vehicle\").\n",
    "\n",
    "5. **Multi-Damage Relations:** Explain how multiple damages relate (e.g., \"scratch running across dent\" or \"cluster of dents\").\n",
    "\n",
    "6. **Component Recognition:** Identify and label both damaged and undamaged car parts for better context.\n",
    "\n",
    "\n",
    "FiftyOne allows you to use VLMs rather seemlessly, for example [Qwen2.5-VL is a VLM](https://github.com/harpreetsahota204/qwen2_5_vl) which has been integrated as a [Remotely Sourced Zoo Model](https://beta-docs.voxel51.com/models/model_zoo/remote/) (which we will discuss later).\n",
    "\n",
    "Start by registering the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "foz.register_zoo_model_source(\"https://github.com/harpreetsahota204/qwen2_5_vl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can download a checkpoint. Refer to the [Qwen2.5-VL's Remote Zoo Model's GitHub](https://github.com/harpreetsahota204/qwen2_5_vl/tree/main) to see the available checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/qwen2_5_vl\",\n",
    "    model_name=\"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can load the model as you would any [Built-in Zoo Model](https://beta-docs.voxel51.com/models/model_zoo/models/).\n",
    "\n",
    "Ttart by setting `operation=\"vqa\"`, which we will use to generate answers/captions for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo_model = foz.load_zoo_model(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    operation=\"vqa\"\n",
    "    # install_requirements=True #if you are using for the first time and need to download reuirement,\n",
    "    # ensure_requirements=True #  ensure any requirements are installed before loading the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set the prompt for the model by as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo_model.prompt=\"Complete a damage report for this vehicle in this image. Include details about the damage, including the location and type of damage. If there is no damage, say 'No damage'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then apply the model to your Dataset via [the `apply_model` method](https://beta-docs.voxel51.com/api/fiftyone.core.models.html#apply_model) of the Dataset.\n",
    "\n",
    "Image captioning/VQA typically takes longer other operations. This took ~17 minutes to run on a single NVIDIA RTX 600 Ada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_dataset.apply_model(zoo_model, label_field=\"damage_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the model for classification, for example to get the color of the vehicle. All you have to do is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo_model.operation=\"classify\"\n",
    "zoo_model.prompt = \"What is the color of the damaged vehicle in this image? Please provide the color name.\"\n",
    "hub_dataset.apply_model(zoo_model, label_field=\"vehicle_color\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also classify the location of the damage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo_model.prompt = \"\"\"You are required to report the location of vehicle damage. \\\n",
    "    List all the locations where this vehicle has been damaged. Choose from one or more of the following, or include any location not explicitly listed: \n",
    "    - quarter panel\n",
    "    - driver door\n",
    "    - passenger door\n",
    "    - rear door\n",
    "    - hood\n",
    "    - front bumper\n",
    "    - rear bumper\n",
    "    - quarter panel\n",
    "    - tires\n",
    "    - rim \n",
    "    - wheel well\n",
    "    - window\n",
    "    - windshield     \n",
    "    \n",
    "    Where all the locations where this vehicle has been damaged?\"\"\"\n",
    "\n",
    "hub_dataset.apply_model(zoo_model, label_field=\"damage_location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly explore what the VLM has come up with. For context, let's open the first image and examine the model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.open(hub_dataset.first().filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hub_dataset.first().damage_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_dataset.first().vehicle_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_dataset.first().damage_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also refresh the FiftyOe App and view the model output for each sample in the Dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
