{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/car_dd_dataset_workshop/blob/main/06_using_3d_models.ipynb)\n",
    "\n",
    "\n",
    "Note: If using in Google Colab, make sure you [install all the requirements listed here](https://github.com/harpreetsahota204/car_dd_dataset_workshop/blob/main/requirements.txt).\n",
    "\n",
    "Which you can do by running the following:\n",
    "\n",
    "```\n",
    "# Install requirements directly from the URL\n",
    "!pip install -r https://raw.githubusercontent.com/harpreetsahota204/car_dd_dataset_workshop/refs/heads/main/requirements.txt\n",
    "```\n",
    "\n",
    "You'll also want to connect to a GPU runtime.\n",
    "\n",
    "\n",
    "\n",
    "# Using VGGT as a FiftyOne Remote Source Zoo Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# dataset = fo.load_dataset(\"cardd_from_hub\")\n",
    "\n",
    "# # or if you are in a new notebook\n",
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"harpreetsahota/cardd_workshop_post_03\",\n",
    "    overwrite=True\n",
    "    )\n",
    "\n",
    "dataset.persistent=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<strong>Note:</strong> This notebook will be faster if you have access to GPU resources. The VGGT model is *relatively* lightweight (you can run it on CPU). But, it may take a while. \n",
    "\n",
    "If you want, you can download a dataset with all the generated 3D assets from the VGGT model, plus the enrichments from the previous notebook via the following:\n",
    "</div>\n",
    "\n",
    "You'll note we have to use a slightly different pattern for downloading 3D assets from Hugging Face. Let's just say we're working on making sure paths are working properly in FO3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download the dataset snapshot to the current working directory\n",
    "\n",
    "snapshot_download(repo_id=\"harpreetsahota/cardd_workshop_post_threed\", local_dir=\"./cardd_threed\", repo_type=\"dataset\")\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "import fiftyone as fo\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load dataset from current directory using FiftyOne's native format\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=\"./cardd_threed\",  # Current directory contains the dataset files\n",
    "    dataset_type=fo.types.FiftyOneDataset,  # Specify FiftyOne dataset format\n",
    "    name=\"cardd_threed\"  # Assign a name to the dataset for identification\n",
    ")\n",
    "\n",
    "def update_dataset_ply_paths(dataset):\n",
    "    \"\"\"\n",
    "    Update PLY file paths in FiftyOne 3D dataset to use absolute paths.\n",
    "    \n",
    "    This function iterates through all samples in a FiftyOne dataset and modifies\n",
    "    any PLY mesh file paths found in the JSON metadata to be absolute paths\n",
    "    relative to the sample's directory location.\n",
    "    \n",
    "    Args:\n",
    "        dataset (fiftyone.Dataset): A FiftyOne dataset containing 3D samples\n",
    "                                   with JSON metadata files that may reference\n",
    "                                   PLY mesh files with relative paths.\n",
    "    \n",
    "    Returns:\n",
    "        None: The function modifies the JSON files in-place on disk.\n",
    "    \n",
    "    Note:\n",
    "        - This function assumes each sample's filepath points to a JSON file\n",
    "        - The JSON structure should contain a 'children' array with objects\n",
    "        - PLY mesh objects are identified by '_type': 'PlyMesh'\n",
    "        - Only objects with a 'plyPath' field will be updated\n",
    "    \"\"\"\n",
    "    # Iterate through each sample in the dataset\n",
    "    for sample in dataset:\n",
    "        # Get the file path of the current sample (JSON metadata file)\n",
    "        fo3d_filepath = sample.filepath\n",
    "        \n",
    "        # Extract the directory containing the sample file\n",
    "        fo3d_directory = os.path.dirname(fo3d_filepath)\n",
    "        \n",
    "        # Read and parse the JSON metadata file\n",
    "        with open(fo3d_filepath, 'r') as f:\n",
    "            fo3d_data = json.load(f)\n",
    "        \n",
    "        # Process each child object in the JSON structure\n",
    "        for child in fo3d_data.get('children', []):\n",
    "            # Check if this child is a PLY mesh object with a path reference\n",
    "            if child.get('_type') == 'PlyMesh' and 'plyPath' in child:\n",
    "                # Convert relative PLY path to absolute path\n",
    "                # by joining it with the sample's directory\n",
    "                child['plyPath'] = os.path.join(fo3d_directory, child['plyPath'])\n",
    "        \n",
    "        # Write the updated JSON data back to the file\n",
    "        with open(fo3d_filepath, 'w') as f:\n",
    "            json.dump(fo3d_data, f, indent=2)  # Pretty-print with 2-space indentation\n",
    "\n",
    "# Execute the path update function on the loaded dataset\n",
    "update_dataset_ply_paths(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model source\n",
    "\n",
    "First, you need to register the model source. You can do so as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/vggt\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to instantiate the model.\n",
    "\n",
    "Note, be sure to install the following:\n",
    "\n",
    "- `vggt@git+https://github.com/facebookresearch/vggt.git`\n",
    "- `open3d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = foz.load_zoo_model(\n",
    "    \"facebook/VGGT-1B\",\n",
    "    mode=\"crop\", # you can also pass \"pad\",\n",
    "    confidence_threshold=0.7\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can apply the model to your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(model, \"depth_map_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we are saving only the paths to the depth map as a dummy field. We won't have these as a part of our original dataset, instead we will create a Grouped Dataset (shown below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get filepaths from your existing dataset\n",
    "filepaths = dataset.values(\"filepath\")\n",
    "\n",
    "# Create a new grouped dataset\n",
    "grouped_dataset = fo.Dataset(\"cardd_3d\", overwrite=True)\n",
    "grouped_dataset.add_group_field(\"group\", default=\"rgb\")\n",
    "\n",
    "# Process each filepath and create the group structure\n",
    "samples = []\n",
    "for filepath in filepaths:\n",
    "    # Extract base information from the filepath\n",
    "    path = Path(filepath)\n",
    "    base_dir = path.parent\n",
    "    base_name = path.stem\n",
    "    \n",
    "    # Create paths for each modality following your pattern\n",
    "    rgb_path = filepath  # Original filepath (RGB)\n",
    "    depth_path = os.path.join(base_dir, f\"{base_name}_depth.png\")  # Depth map\n",
    "    threed_path = os.path.join(base_dir, f\"{base_name}.fo3d\")  # 3D point cloud\n",
    "    \n",
    "    # Create a group for these related samples\n",
    "    group = fo.Group()\n",
    "    \n",
    "    # Create a sample for each modality with the appropriate group element\n",
    "    rgb_sample = fo.Sample(filepath=rgb_path, group=group.element(\"rgb\"))\n",
    "    depth_sample = fo.Sample(filepath=depth_path, group=group.element(\"depth\"))\n",
    "    threed_sample = fo.Sample(filepath=threed_path, group=group.element(\"threed\"))\n",
    "    \n",
    "    # Add samples to the list\n",
    "    samples.extend([rgb_sample, depth_sample, threed_sample])\n",
    "\n",
    "# Add all samples to the dataset\n",
    "grouped_dataset.add_samples(samples)\n",
    "grouped_dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can view the results in the FiftyOne App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(grouped_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
