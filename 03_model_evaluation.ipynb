{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/car_dd_dataset_workshop/blob/main/03_model_evaluation.ipynb)\n",
    "\n",
    "Note: If using in Google Colab, make sure you [install all the requirements listed here](https://github.com/harpreetsahota204/car_dd_dataset_workshop/blob/main/requirements.txt).\n",
    "\n",
    "# Model evaluation in FiftyOne\n",
    "\n",
    "Before we can evaluate model performance, we need to make some predictions!\n",
    "\n",
    "Let's start by loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset = fo.load_dataset(\"cardd_from_hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # or if you are in a new notebook\n",
    "# import fiftyone as fo\n",
    "# from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "# dataset = load_from_hub(\n",
    "#     \"harpreetsahota/CarDD\",\n",
    "#     name=\"cardd_from_hub\",\n",
    "#     max_samples=100, # if you want to work with a subset of the dataset\n",
    "#     persistent=True,\n",
    "#     overwrite=True,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the ground truth labels for our bounding boxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=dataset.distinct(\"detections.detections.label\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with some zero-shot models\n",
    "\n",
    "### Using the Hugging Face Integration\n",
    "\n",
    "We can use our [integration with Hugging Face](https://docs.voxel51.com/integrations/huggingface.html#image-classification) to generate some predictions using a zero shot object detection model.\n",
    "\n",
    "In this case, let's use a version of [the OWL model by Google](https://huggingface.co/google/owlv2-base-patch16-finetuned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "owl_model = foz.load_zoo_model(\n",
    "    \"zero-shot-detection-transformer-torch\",\n",
    "    name_or_path=\"google/owlv2-base-patch16-finetuned\",  # HF model name or path\n",
    "    classes=classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(\n",
    "    owl_model, \n",
    "    label_field=\"owlvit_pred\", \n",
    "    confidence_thresh=0.2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultralytics Integration\n",
    "\n",
    "You can also use FiftyOne's [integration with Ultralytics](https://docs.voxel51.com/integrations/ultralytics.html) to generate some predictions.\n",
    "\n",
    "Let's use the [YOLO-World](https://docs.ultralytics.com/models/yolo-world) model for open vocabulary segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "yolo_model = foz.load_zoo_model(\n",
    "    \"yoloe11l-seg-torch\",\n",
    "    classes=classes,\n",
    ")\n",
    "\n",
    "dataset.apply_model(\n",
    "    yolo_model, \n",
    "    label_field=\"yolo_seg\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a Vision Language Model - PaliGemma2-Mix\n",
    "\n",
    "\n",
    "Note: PaliGemma reuiqres `flax` and `jax`\n",
    "\n",
    "> You can also use Florence2! Check out the [GitHub repo for the integration here](https://github.com/harpreetsahota204/florence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "foz.register_zoo_model_source(\"https://github.com/harpreetsahota204/paligemma2\", overwrite=True)\n",
    "\n",
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/paligemma2\",\n",
    "    model_name=\"google/paligemma2-3b-mix-224\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paligemma_model = foz.load_zoo_model(\n",
    "    \"google/paligemma2-3b-mix-224\",\n",
    "    # install_requirements=True #if you are using for the first time and need to download reuirement,\n",
    "    # ensure_requirements=True #  ensure any requirements are installed before loading the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set operation and classes to segment\n",
    "paligemma_model.operation = \"detection\"\n",
    "paligemma_model.prompt = classes\n",
    "\n",
    "# Apply to dataset\n",
    "dataset.apply_model(paligemma_model, label_field=\"pg_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set operation and classes to segment\n",
    "paligemma_model.operation = \"segment\"\n",
    "paligemma_model.prompt = classes\n",
    "\n",
    "# Apply to dataset\n",
    "dataset.apply_model(paligemma_model, label_field=\"pg_segmentations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `evaluate_detections()` to Analyze Your Object Detection Models\n",
    "\n",
    "FiftyOne's [`evaluate_detections()`](https://docs.voxel51.com/user_guide/evaluation.html#supported-types) method gives you powerful insights into how well your object detection models are performing.\n",
    "\n",
    "This method compares your model's predicted bounding boxes against ground truth annotations, calculating metrics like precision, recall, and mAP while also providing sample-level statistics that help you identify exactly where your model struggles. You can evaluate any detection task including standard object detection, instance segmentation, polygon detection, keypoints, and even temporal detections in videos. \n",
    "\n",
    "The evaluation uses COCO-style evaluation by default, but you can also switch to Open Images-style or ActivityNet-style evaluation depending on your needs.\n",
    "\n",
    "When you specify an `eval_key` parameter it populates helpful fields on each sample and detection that you can explore interactively in the FiftyOne App.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions against ground truth\n",
    "box_results = dataset.evaluate_detections(\n",
    "    \"yolo_seg\",           # field containing model predictions  \n",
    "    gt_field=\"detections\", # field containing ground truth\n",
    "    eval_key=\"eval\",        # key to store evaluation results\n",
    "    compute_mAP=True,        # compute mean Average Precision\n",
    "    use_boxes=True,\n",
    "    method=\"coco\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed metrics\n",
    "box_results.print_report()\n",
    "print(f\"mAP: {box_results.mAP():.3f}\")\n",
    "\n",
    "# View false positives in the app\n",
    "from fiftyone import ViewField as F\n",
    "fp_view = dataset.filter_labels(\"yolo_seg\", F(\"eval\") == \"fp\")\n",
    "dataset.save_view(view=fp_view, name=\"fp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `evaluate_detections()` for Instance Segmentation Analysis\n",
    "\n",
    "FiftyOne's `evaluate_detections()` method seamlessly handles instance segmentation when your masks are stored as `Detections` with populated mask fields.\n",
    "\n",
    "When you have segmentation masks stored within `Detection` objects, the evaluation automatically switches to using mask-based IoU calculations instead of bounding box IoU by setting `use_masks=True`. This gives you precise pixel-level overlap measurements between predicted and ground truth instance masks while still providing all the standard detection evaluation metrics like precision, recall, and mAP. The method follows the same COCO-style evaluation protocol but calculates IoU based on the actual segmented regions rather than just rectangular bounding boxes.\n",
    "\n",
    "You get the best of both worlds - instance-level evaluation with pixel level accuracy measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions against ground truth\n",
    "mask_results = dataset.evaluate_detections(\n",
    "    \"yolo_seg\",           # field containing model predictions  \n",
    "    gt_field=\"segmentations\", # field containing ground truth\n",
    "    eval_key=\"seg_evals\",     # key to store evaluation results\n",
    "    use_masks=True,         # Use pixel masks for IoU calculation\n",
    "    compute_mAP=True,        # compute mean Average Precision\n",
    "    method=\"coco\",\n",
    "    # iou=0.50           #you can set this threshold, it defualts to 0.50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print segmentation-specific metrics\n",
    "mask_results.print_report()\n",
    "print(f\"Mask-based mAP: {mask_results.mAP():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "poor_masks = dataset.filter_labels(\"yolo_seg\", \n",
    "                                   (F(\"eval\") == \"tp\") & (F(\"eval_iou\") < 0.5))\n",
    "\n",
    "dataset.save_view(view=poor_masks, name=\"poor_masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Mask Quality vs Bounding Box Performance\n",
    "\n",
    "You can run both mask-based and box-based evaluations on the same dataset to understand where your model excels at detection versus precise segmentation.\n",
    "\n",
    "Running separate evaluations with `use_masks=True` and `use_masks=False` reveals interesting insights about your instance segmentation model's behavior. A detection might be correctly localized (good bounding box IoU) but have poor mask quality, or conversely, have excellent mask precision within a slightly misplaced bounding box. This dual analysis helps you understand whether your model's weaknesses lie in object localization, boundary delineation, or both.\n",
    "\n",
    "The sample-level fields from both evaluations let you identify specific failure modes and edge cases.\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two approaches\n",
    "print(\"Mask-based mAP:\", mask_results.mAP())\n",
    "print(\"Box-based mAP:\", box_results.mAP())\n",
    "\n",
    "# Find samples where box detection succeeded but mask quality failed\n",
    "from fiftyone import ViewField as F\n",
    "mask_failures = dataset.filter_labels(\n",
    "    \"yolo_seg\",\n",
    "    (F(\"box_eval\") == \"tp\") & (F(\"mask_eval\") == \"fp\")\n",
    ")\n",
    "\n",
    "dataset.save_view(mask_failures, \"mask_failures\")\n",
    "\n",
    "print(f\"Found {mask_failures.count('predictions.detections')} instances with good boxes but poor masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's install a panel to make it easy for us to understand overall model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download \\\n",
    "    https://github.com/voxel51/fiftyone-plugins \\\n",
    "    --plugin-names @voxel51/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune and evaluate a model\n",
    "\n",
    "### Split dataset\n",
    "\n",
    "Let's start by splitting our dataset, which we can with the [built-in utility functions in FiftyOne](https://docs.voxel51.com/api/fiftyone.utils.random.html#fiftyone.utils.random.random_split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.random as four\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "four.random_split(dataset, {\"train\": 0.8, \"val\": 0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.skip(51).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also go ahead and load in the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "test_dataset = fo.Dataset.from_dir(\n",
    "    data_path=\"CarDD_release/CarDD_COCO/test2017\",\n",
    "    labels_path=\"CarDD_release/CarDD_COCO/annotations/instances_test2017.json\",\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    name=\"car_dd_test\",\n",
    "    overwrite=True,\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FiftyOne's [`match_tags()` method](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.match_tags) lets you easily create views for each split after using `random_split()`.\n",
    "\n",
    "When you use `random_split()`, FiftyOne automatically adds tags like \"train\", \"test\", and \"val\" to your samples based on the split ratios you specified. You can then create filtered views of your dataset by matching these tags, which is perfect for running separate evaluations on your training, validation, and test sets. This approach lets you analyze how your model performs across different data splits and catch issues like overfitting or distribution shifts between splits.\n",
    "\n",
    "Each view acts like a subset of your original dataset while maintaining all the same functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_view = dataset.match_tags(\"train\")\n",
    "val_view = dataset.match_tags(\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky Split Detector\n",
    "\n",
    "This code finds data contamination between training/validation/test splits in machine learning datasets.\n",
    "\n",
    "When you're building ML models, you split your data into different buckets - like training, validation, and test sets. But sometimes the same image (or very similar images) accidentally end up in multiple buckets, which is called \"data leakage\" and can make your model performance metrics totally bogus. [This tool uses image similarity to hunt down these sneaky duplicates that are hiding across your splits](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_leaky_splits).\n",
    "\n",
    "This method computes embeddings for all your images and builds a similarity index to find near-duplicates.\n",
    "\n",
    "First, it runs your images through a neural network (default is ResNet-18, but you can change it) to get feature embeddings that capture what each image looks like.  Then, it uses these embeddings to build a similarity index that can quickly find which images are suspiciously similar to each other.  When you call `find_leaks()` with a threshold, it identifies pairs of similar images that live in different splits and flags them as potential leaks.\n",
    "\n",
    "You can tag leaky samples, exclude them from views, or drill down into specific leak relationships.\n",
    "\n",
    "Yu can automatically tag all the problematic samples, create filtered views that exclude the leaks entirely, or investigate specific samples to see all their similar neighbors across splits. It also warns you about overlapping splits and samples that couldn't be processed, so you know exactly what's going on with your data quality.\n",
    "\n",
    "You can learn more about [this here](https://voxel51.com/blog/on-leaky-datasets-and-a-clever-horse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.brain import compute_leaky_splits\n",
    "\n",
    "leaks_index = compute_leaky_splits(\n",
    "    dataset, \n",
    "    splits=['train', \"val\"], \n",
    "    model=\"clip-vit-base32-torch\"\n",
    "    )\n",
    "\n",
    "leaks = leaks_index.leaks_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataset to YOLO format\n",
    "\n",
    "FiftyOne supports [exporting datasets to disk in various common formats](https://docs.voxel51.com/user_guide/export_datasets.html) and can be extended for custom formats.\n",
    "\n",
    " You can export entire datasets or subsets using the Python library or CLI. The `export()` method automatically coerces data types to match the requested export types. Several dataset types are supported, including:\n",
    " \n",
    " *  `ImageDirectory`\n",
    " *  `VideoDirectory`\n",
    " *  `FiftyOneImageClassificationDataset`\n",
    " *  `COCODetectionDataset`\n",
    " \n",
    " You can also export datasets in custom formats by defining your own Dataset or DatasetExporter class.\n",
    "\n",
    "\n",
    "We're going to fine-tune a YOLO model, so we'll make use of the [YOLO exporter.](https://docs.voxel51.com/user_guide/export_datasets.html#yolov5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import fiftyone.utils.ultralytics as fou\n",
    "import fiftyone.utils.labels as fol\n",
    "\n",
    "EXPORT_DIR = \"/tmp/car_dd_yolo\"\n",
    "\n",
    "YAML_FILE = os.path.join(EXPORT_DIR, \"dataset.yaml\")\n",
    "\n",
    "fol.instances_to_polylines(dataset, \"segmentations\", \"polylines\")\n",
    "\n",
    "train_view.export(\n",
    "    export_dir=EXPORT_DIR,\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    label_field=\"polylines\",\n",
    "    split=\"train\",\n",
    "    classes=classes,\n",
    ")\n",
    "\n",
    "val_view.export(\n",
    "    export_dir=EXPORT_DIR,\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    label_field=\"polylines\",\n",
    "    split=\"val\",  # Ultralytics uses 'val'\n",
    "    classes=classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolo11s-seg.pt')  # Pre-trained YOLOv11 segmentation model\n",
    "\n",
    "# Set up training configuration\n",
    "model.train(\n",
    "    data=YAML_FILE,\n",
    "    epochs=100,                # Increased for effective learning\n",
    "    batch=8,                   # Adjust if increasing imgsz\n",
    "    imgsz=1024,                # Higher for small object segmentation\n",
    "    val=True,\n",
    "    save=True,\n",
    "    save_period=10,\n",
    "    project='car_damage_segmentation',\n",
    "    name='yolov11_seg_run',\n",
    "    workers=8,                 # If hardware allows\n",
    "    device='cuda',\n",
    "    patience=15,               # Slightly more patience for complex data\n",
    "    verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply this fine-tuned model to our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./car_damage_segmentation/yolov11_seg_run/weights/best.pt\"\n",
    "\n",
    "model = YOLO(model_path)\n",
    "\n",
    "test_dataset.apply_model(model, label_field=\"fine_tuned_yolo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions against ground truth\n",
    "ft_results = test_dataset.evaluate_detections(\n",
    "    \"fine_tuned_yolo\",           # field containing model predictions  \n",
    "    gt_field=\"segmentations\", # field containing ground truth\n",
    "    eval_key=\"ft_evals\",     # key to store evaluation results\n",
    "    use_masks=True,         # Use pixel masks for IoU calculation\n",
    "    compute_mAP=True,        # compute mean Average Precision\n",
    "    method=\"coco\",\n",
    "    # iou=0.50           #you can set this threshold, it defualts to 0.50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view that contains only high confidence false positive model\n",
    "# predictions, with samples containing the most false positives first\n",
    "most_fp_view = (\n",
    "    test_dataset\n",
    "    .filter_labels(\"fine_tuned_yolo\", (F(\"confidence\") > 0.8) & (F(\"eval\") == \"fp\"))\n",
    "    .sort_by(F(\"fine_tuned_yolo.detections\").length(), reverse=True)\n",
    ")\n",
    "\n",
    "test_dataset.save(most_fp_view, \"most_fp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistakenness\n",
    "\n",
    "You can [use FiftyOne to calculate a mistakenness score for your ground truth labels](https://docs.voxel51.com/brain.html#brain-label-mistakes). This algorithm finds potential annotation errors by checking when confident model predictions disagree with your ground truth labels.\n",
    "\n",
    "The core idea is simple: if your model is really confident about a prediction but your ground truth says something different, there's probably a labeling mistake. The algorithm calculates a \"mistakenness score\" that's high when the model is confident and wrong, suggesting the ground truth might be incorrect rather than the model. It works for both classification (wrong class label) and localization (wrong bounding box position).\n",
    "\n",
    "This helps you clean up datasets by automatically flagging suspicious annotations for human review.\n",
    "\n",
    "### How the Scoring Works\n",
    "\n",
    "The mistakenness score combines model confidence with whether the prediction matches the ground truth label.\n",
    "\n",
    "For classification, the formula is `(m * exp(-entropy(logits)) + 1) / 2` where `m` equals -1 for correct predictions and +1 for incorrect ones. The entropy part measures model confidence - lower entropy means higher confidence. So when the model is confident and wrong (high confidence, m=+1), you get a high mistakenness score. When the model is confident and correct (high confidence, m=-1), you get a low score.\n",
    "\n",
    "For **localization mistakenness**, it uses a different formula that considers both confidence and IoU: `(c * ((2 * i) - 1) + 1) / 2` where `c` is the model confidence and `i` represents how bad the localization is (0 for perfect IoU, 1 for terrible IoU at the 0.5 threshold). This means objects with poor bounding box alignment get higher localization mistakenness scores, especially when the model was confident about the detection.\n",
    "\n",
    "### Three Types of Issues It Finds\n",
    "\n",
    "Thiss will help you identify three categories of potential problems: incorrect labels, missing annotations, and spurious annotations.\n",
    "\n",
    "1) **Incorrect labels** happen when ground truth objects have matching predictions but disagree on class or location - these get mistakenness scores to rank how suspicious they are. \n",
    "\n",
    "2) **Missing annotations** are high-confidence predictions (above 95% confidence) that don't match any ground truth object, suggesting the annotator missed something obvious. \n",
    "\n",
    "3) **Spurious annotations** are ground truth objects that don't match any prediction, indicating they might be phantom labels or incorrectly placed.\n",
    "\n",
    "Each category helps you focus on different types of annotation quality issues.\n",
    "\n",
    "### The Detection Pipeline\n",
    "\n",
    "[For object detection](https://docs.voxel51.com/tutorials/detection_mistakes.html), it first matches predictions to ground truth using IoU overlap, then computes mistakenness for each matched pair.\n",
    "\n",
    "The process starts by running standard detection evaluation with a 0.5 IoU threshold to pair up predicted and ground truth objects. For each matched pair, it calculates both classification mistakenness (wrong class) and localization mistakenness (wrong position). Unmatched high-confidence predictions become missing annotation candidates, while unmatched ground truth objects become spurious annotation candidates. The final sample-level score is just the highest mistakenness of any object in that image.\n",
    "\n",
    "This matching step is crucial because it establishes which predictions correspond to which annotations before judging their quality.\n",
    "\n",
    "\n",
    "### Note: Pay attention that we are now going to apply our trained model to the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"./car_damage_segmentation/yolov11_seg_run/weights/best.pt\"\n",
    "\n",
    "# model = YOLO(model_path)\n",
    "\n",
    "dataset.apply_model(model, label_field=\"fine_tuned_yolo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "# Compute mistakenness of annotations in `segmentations` field using\n",
    "# predictions from `predictions` field as point of reference\n",
    "fob.compute_mistakenness(dataset, \"fine_tuned_yolo\", label_field=\"segmentations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Gets Added to Your Dataset\n",
    "\n",
    "The mistakenness analysis adds new fields to your ground truth objects, predicted objects, and samples to flag potential annotation issues.\n",
    "\n",
    "**Ground truth objects** get three new attributes: \n",
    "\n",
    "- `mistakenness` scores how likely the class label is wrong\n",
    "- `mistakenness_loc` scores how likely the bounding box position is wrong\n",
    "- `possible_spurious` marks objects that probably shouldn't exist (no matching prediction found). \n",
    "\n",
    "**Predicted objects** get one new attribute: \n",
    "\n",
    "- `possible_missing` flags high-confidence predictions that have no ground truth match, suggesting the annotator missed labeling an obvious object.\n",
    "\n",
    "**Sample-level fields** summarize the issues: \n",
    "\n",
    "- `mistakenness` takes the highest mistakenness score from any ground truth object in that image\n",
    "- `possible_spurious` and `possible_missing` count how many problematic objects were found in each sample.\n",
    "\n",
    "\n",
    "This gives you both fine-grained object-level feedback and high-level sample summaries to prioritize your annotation review.\n",
    "\n",
    "###  How to Use These Flags\n",
    "\n",
    "You can sort and filter your dataset using these new fields to focus on the most problematic annotations first.\n",
    "\n",
    "Sort samples by the `mistakenness` field to see images with the most suspicious ground truth labels at the top. Filter for samples with high `possible_missing` counts to find images where your annotators likely missed obvious objects. Use `possible_spurious` counts to identify samples with phantom annotations that probably don't belong.\n",
    "\n",
    "At the object level, you can filter ground truth objects by `mistakenness > 0.8` to see the most questionable class labels, or `mistakenness_loc > 0.7` to find bounding boxes that are probably in the wrong place.\n",
    "\n",
    "This approach helps you tackle annotation quality issues in order of severity rather than randomly reviewing your entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Sort by likelihood of mistake (most likely first)\n",
    "mistake_view = dataset.sort_by(\"mistakenness\", reverse=True)\n",
    "\n",
    "dataset.save_view(mistake_view, \"sorted_mistake_view\")\n",
    "\n",
    "# Print some information about the view\n",
    "print(mistake_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_mistaken_score = dataset.filter_labels(\"segmentations\", F(\"mistakenness\") > 0.7)\n",
    "\n",
    "dataset.save_view(high_mistaken_score,\"high_mistaken_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooly_localized = dataset.filter_labels(\"segmentations\", F(\"mistakenness_loc\") > 0.85)\n",
    "dataset.save_view(pooly_localized, \"pooly_localized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take this a step further and fix the errors in the labels!\n",
    "\n",
    "To do this we will use [FiftyOne's integration with CVAT](https://docs.voxel51.com/integrations/cvat.html). You can sign up [here](https://app.cvat.ai). I recommend signing up with an email and setting your password explicitly, rather than using a third-party sign on.\n",
    "\n",
    "Once you've done that, set the following environment variables:\n",
    "- `FIFTYONE_CVAT_USERNAME`\n",
    "- `FIFTYONE_CVAT_PASSWORD`\n",
    "\n",
    "\n",
    "Note: FiftyOne also integrats with [Label Studio](https://docs.voxel51.com/integrations/index.html), [V7](https://docs.voxel51.com/integrations/v7.html), and [Labelbox](https://docs.voxel51.com/integrations/labelbox.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ['FIFTYONE_CVAT_USERNAME'] = getpass(\"Enter your CVAT user name:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['FIFTYONE_CVAT_PASSWORD'] = getpass(\"Enter your CVAT password:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can [learn more about annotations](https://docs.voxel51.com/user_guide/annotation.html?) in FiftyOne here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_mistaken_score.annotate(\n",
    "    anno_key=\"high_mistaken_score\",\n",
    "    backend=\"cvat\",\n",
    "    label_field=\"segmentations\",\n",
    "    label_type=\"instances\",\n",
    "    classes=dataset.default_classes,\n",
    "    allow_additions=True,\n",
    "    allow_deletions=True,\n",
    "    allow_label_edits=True,\n",
    "    launch_editor=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done the annotation work, we can load them back into FiftyOne!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_mistaken_score.load_annotations(\"high_mistaken_score\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
