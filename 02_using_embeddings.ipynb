{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Embeddings for Deeper Dataset Understanding\n",
    "\n",
    "You can use embeddings to gain a deeper understanding of the images in this dataset.\n",
    "Visual embeddings can help analyze car damage images in several key ways:\n",
    "\n",
    "1. **Relationship Visualization:** Using dimensionality reduction (like UMAP) to visualize how different damage types cluster together and identify patterns.\n",
    "\n",
    "2. **Model Comparison:** Compare how different vision models encode and interpret car damage, revealing their unique perspectives and strengths.\n",
    "\n",
    "3. **Category Analysis:** Explore visual similarities and differences between the six damage types (dent, scratch, crack, glass shatter, tire flat, lamp broken).\n",
    "\n",
    "4. **Variation Study:** Understand how factors like shooting angle and vehicle color affect damage representation in embedding space.\n",
    "\n",
    "5. **Feature Detection:** Identify subtle visual features that distinguish different types of damage, which might not be obvious in the annotations.\n",
    "\n",
    "For this analysis, we’ll use these models:\n",
    "\n",
    "• CLIP\n",
    "\n",
    "• SigLIP 2\n",
    "\n",
    "Note that both of these models can be used for zero-shot classification. We won't discuss their use for that task here, but you're encouraged to [learn more about zero-shot classification in this tutorial](https://github.com/harpreetsahota204/getting-started-fo-experiences/blob/main/zero-shot-prediction/zero-shot-classification.ipynb).\n",
    "\n",
    "Start by loading the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset = fo.load_dataset(\"cardd_from_hub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you can instantiate the models and then computing embeddings.\n",
    "\n",
    "# Open CLIP Integration\n",
    "\n",
    "FiftyOne [integrates natively with the OpenCLIP library](https://beta-docs.voxel51.com/integrations/openclip/), an open source implementation of OpenAI’s CLIP (Contrastive Language-Image Pre-training) model that you can use to run inference on your FiftyOne datasets with a few lines of code!\n",
    "\n",
    "To use models from OpenCLIP you need to ensure you have installed the `open_clip_torch` package (which is part of the `requirements.txt` for this workshop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "clip_model = foz.load_zoo_model(\n",
    "    \"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"openai/clip-vit-base-patch32\", \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    use_fast=True,\n",
    "    # install_requirements=True # uncomment this line if you are running this code for the first time\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify model architectures and pretrained weights by passing in optional parameters. Pretrained models can be loaded directly from OpenCLIP with the following syntax:\n",
    "\n",
    "\n",
    "```python\n",
    "meta_clip = foz.load_zoo_model(\n",
    "    name_or_url=\"open-clip-torch\",\n",
    "    clip_model=\"ViT-B-32-quickgelu\",\n",
    "    pretrained=\"metaclip_400m\",\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "Alternatively you can also load a model from Hugging Face’s Model Hub with the following syntax:\n",
    "\n",
    "\n",
    "```python\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "open_clip_model = foz.load_zoo_model(\n",
    "    name_or_url=\"open-clip-torch\",\n",
    "    clip_model=\"hf-hub:repo-name/model-name\",\n",
    "    pretrained=\"\",\n",
    ")\n",
    "```\n",
    "\n",
    "As a concrete example, if you were interested in the [StreetCLIP model](https://huggingface.co/geolocal/StreetCLIP) you would use:\n",
    "\n",
    "```python\n",
    "street_clip_model = foz.load_zoo_model(\n",
    "    name_or_url=\"open-clip-torch\",\n",
    "    pretrained=\"\",\n",
    "    clip_model=\"hf-hub:geolocal/StreetCLIP\"\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hugging Face Integration\n",
    "\n",
    "\n",
    "You can also run models from Hugging Face as a Zoo Model with [FiftyOne's Hugging Face Integration](https://beta-docs.voxel51.com/integrations/huggingface/#zero-shot-classification). \n",
    "\n",
    "To load a model from the Hugging Face Hub, set `name_or_url=zero-shot-classification-transformer-torch`. This specifies that you want to a zero-shot image classification model from the Hugging Face Transformers library. You can then specify the model via the `name_or_path` argument. This should be the repository name or model identifier of the model you want to load:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "siglip_model = foz.load_zoo_model(\n",
    "    \"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"google/siglip2-so400m-patch14-384\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    use_fast=True,\n",
    "    # install_requirements=True # uncomment this line if you are running this code for the first time\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing embeddings\n",
    "\n",
    "We can use [the `compute_embeddings`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#compute_embeddings) method of the Dataset as follows to compute embeddings for the images in the Dataset.\n",
    "\n",
    "This method supports all the following cases:\n",
    "\n",
    "• Using an image model to compute image embeddings for an image collection\n",
    "\n",
    "• Using an image model to compute frame embeddings for a video collection\n",
    "\n",
    "• Using a video model to compute embeddings for a video collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 374/374 [9.3s elapsed, 0s remaining, 39.6 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=clip_model,\n",
    "    embeddings_field=\"clip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 374/374 [18.2s elapsed, 0s remaining, 20.8 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=siglip_model,\n",
    "    embeddings_field=\"siglip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Visualization\n",
    "\n",
    "Now, we can use UMAP to reduce the dimensionality of the embeddings and explore them in the FiftyOne app. \n",
    "\n",
    "> Note that you will need to have the `umap-learn` package installed for this, which is also listed as a requirement in the `requirements.txt` file of this repository.\n",
    "\n",
    "We can use the [FiftyOne Brain](https://beta-docs.voxel51.com/fiftyone_concepts/brain/) to perform [dimensionality reduction](https://beta-docs.voxel51.com/tutorials/dimension_reduction/) so that we can viualize the embeddings in the FiftyOne App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n",
      "UMAP( verbose=True)\n",
      "Mon Apr  7 17:37:14 2025 Construct fuzzy simplicial set\n",
      "Mon Apr  7 17:37:14 2025 Finding Nearest Neighbors\n",
      "Mon Apr  7 17:37:14 2025 Finished Nearest Neighbor Search\n",
      "Mon Apr  7 17:37:14 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fccb7d1a3e74baa96cef017ab97526d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Mon Apr  7 17:37:14 2025 Finished embedding\n",
      "Generating visualization...\n",
      "UMAP( verbose=True)\n",
      "Mon Apr  7 17:37:14 2025 Construct fuzzy simplicial set\n",
      "Mon Apr  7 17:37:14 2025 Finding Nearest Neighbors\n",
      "Mon Apr  7 17:37:14 2025 Finished Nearest Neighbor Search\n",
      "Mon Apr  7 17:37:14 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74ea7ff145941fab2b89d15e8999389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Mon Apr  7 17:37:14 2025 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "embedding_fields = [\n",
    "    \"clip_embeddings\",\n",
    "    \"siglip_embeddings\"\n",
    "]\n",
    "\n",
    "# Compute UMAP for each embedding\n",
    "\n",
    "for field in embedding_fields:\n",
    "    _fname = field.split(\"_embeddings\")[0]\n",
    "    brain_key = f\"{_fname}_viz\"\n",
    "    \n",
    "    results = fob.compute_visualization(\n",
    "        dataset,\n",
    "        embeddings=field,\n",
    "        method=\"umap\",\n",
    "        brain_key=brain_key,\n",
    "        num_dims=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Embeddings\n",
    "\n",
    "You can launch the app in a notebook by running:\n",
    "\n",
    "```python\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "fo.launch_app(hub_dataset)\n",
    "```\n",
    "\n",
    "Or, you can open your terminal and execute `fiftyone app launch`. This will open the App in a browser window. The you will select your Dataset from the dropdown menu, open the embeddings panel by clicking the `+`  next to the Samples viewer, and select the embeddings you want to display by selecting from the dropdown menu in the embeddings panel.\n",
    "\n",
    "## Patch embeddings\n",
    "\n",
    "Note that these embeddings are computed for the entire image. You may find it interesting and useful to compute embeddings for each *patch* of an image. That is, each segmentation mask in the Dataset.\n",
    "\n",
    "You can use the more explicit pattern defined above where we we:\n",
    "\n",
    "- instantiated a model\n",
    "\n",
    "- called the `compute_embeddings` method of the Dataset with that model\n",
    "\n",
    "- called FiftyOne Brain's `compute_visualization` method with those embeddings\n",
    "\n",
    "However, you can directly compute and visualize embeddings with any Zoo Model using the following pattern:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing patch embeddings...\n",
      " 100% |█████████████████| 374/374 [13.6s elapsed, 0s remaining, 27.2 samples/s]      \n",
      "Generating visualization...\n",
      "UMAP( verbose=True)\n",
      "Mon Apr  7 17:37:44 2025 Construct fuzzy simplicial set\n",
      "Mon Apr  7 17:37:45 2025 Finding Nearest Neighbors\n",
      "Mon Apr  7 17:37:45 2025 Finished Nearest Neighbor Search\n",
      "Mon Apr  7 17:37:45 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef890ff79aa4ac98ffdcd5dfe865779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Mon Apr  7 17:37:45 2025 Finished embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fiftyone.brain.visualization.VisualizationResults at 0x7c67e0984d50>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    model=\"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"google/siglip2-so400m-patch14-384\",    \n",
    "    patches_field=\"ground_truth\",\n",
    "    brain_key=\"siglip_viz_patches\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize text embeddings\n",
    "\n",
    "You can also compute embeddings for the `damage_report` field we generated using the VLM and visualize those in the App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import fiftyone.brain as fob\n",
    "from transformers import AutoModel\n",
    "\n",
    "#set an environment variable so tokenizers doesn't yell at us,\n",
    "# note this related to the `transformers` and `tokenizers` libraries and not a FiftyOne specific environment variable\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "jina_embeddings_model = AutoModel.from_pretrained(\n",
    "    \"jinaai/jina-embeddings-v3\", \n",
    "    trust_remote_code=True,\n",
    "    device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "for sample in dataset.iter_samples(autosave=True):\n",
    "    text_embeddings = jina_embeddings_model.encode(\n",
    "        sentences = [sample[\"damage_report\"]], # model expects a list of strings\n",
    "        task=\"separation\"\n",
    "        )\n",
    "    sample[\"text_embeddings\"] = text_embeddings.squeeze()\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "        dataset,\n",
    "        embeddings=\"text_embeddings\",\n",
    "        method=\"umap\",\n",
    "        brain_key=f\"text_embeddings_viz\",\n",
    "        num_dims=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, at this point it will be helpful for us to install the [Caption View Plugin](https://github.com/mythrandire/caption-viewer) so that we can more easily read the captions as we explore them in embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search your images with natural language\n",
    "\n",
    "When you create a similarity index powered by the CLIP model, you can also search by arbitrary natural language queries natively in the App.\n",
    "\n",
    "Note: For the models we used to compute emebeddings above, FiftyOne's implementation uses those model to extract image embeddings. So those models don't *currently* support text prompts, hence we use a model whose implementatin in FiftyOne does support both text and images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    brain_key=\"text_img_sim\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will allow you to search images \"holistically\", that is considering the entire content of an image.\n",
    "\n",
    "However, you can also do the create an index on the *patch* level and search that via natural language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "patch_text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    patches_field=\"ground_truth\",\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    brain_key=\"patch_text_img_sim\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick note on managing Brain runs\n",
    "\n",
    "You can keep track of the Brain runs you have by calling:\n",
    "\n",
    "```python\n",
    "dataset.list_brain_runs()\n",
    "```\n",
    "\n",
    "If you were to rerun the cell above again, you'd see the error following error:\n",
    "\n",
    "```python\n",
    "ValueError: Brain method run with key 'patch_text_img_sim' already exists\n",
    "```\n",
    "\n",
    "If you want to ever delete a specific Brain run, for example you really like the name `patch_text_img_sim` and want to use it again,  then you can run:\n",
    "\n",
    "```python\n",
    "dataset.delete_brain_run(\"patch_text_img_sim\")\n",
    "```\n",
    "\n",
    "⚠️ To delete ALL the Brain runs on a Dataset, you can call (though it is not recommended):\n",
    "\n",
    "```python\n",
    "dataset.delete_brain_runs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn to the app and perform some natural language search. Click on the `🔎` icon in the Samples viewer and start searching!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other embeddings based workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
