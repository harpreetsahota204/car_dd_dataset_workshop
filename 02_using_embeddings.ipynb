{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Embeddings for Deeper Dataset Understanding\n",
    "\n",
    "You can use embeddings to gain a deeper understanding of the images in this dataset.\n",
    "Visual embeddings can help analyze car damage images in several key ways:\n",
    "\n",
    "1. **Relationship Visualization:** Using dimensionality reduction (like UMAP) to visualize how different damage types cluster together and identify patterns.\n",
    "\n",
    "2. **Model Comparison:** Compare how different vision models encode and interpret car damage, revealing their unique perspectives and strengths.\n",
    "\n",
    "3. **Category Analysis:** Explore visual similarities and differences between the six damage types (dent, scratch, crack, glass shatter, tire flat, lamp broken).\n",
    "\n",
    "4. **Variation Study:** Understand how factors like shooting angle and vehicle color affect damage representation in embedding space.\n",
    "\n",
    "5. **Feature Detection:** Identify subtle visual features that distinguish different types of damage, which might not be obvious in the annotations.\n",
    "\n",
    "For this analysis, we‚Äôll use these models:\n",
    "\n",
    "‚Ä¢ CLIP\n",
    "\n",
    "‚Ä¢ SigLIP 2\n",
    "\n",
    "Note that both of these models can be used for zero-shot classification. We won't discuss their use for that task here, but you're encouraged to [learn more about zero-shot classification in this tutorial](https://github.com/harpreetsahota204/getting-started-fo-experiences/blob/main/zero-shot-prediction/zero-shot-classification.ipynb).\n",
    "\n",
    "Start by loading the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset = fo.load_dataset(\"cardd_from_hub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you can instantiate the models and then computing embeddings.\n",
    "\n",
    "# Open CLIP Integration\n",
    "\n",
    "FiftyOne [integrates natively with the OpenCLIP library](https://beta-docs.voxel51.com/integrations/openclip/), an open source implementation of OpenAI‚Äôs CLIP (Contrastive Language-Image Pre-training) model that you can use to run inference on your FiftyOne datasets with a few lines of code!\n",
    "\n",
    "To use models from OpenCLIP you need to ensure you have installed the `open_clip_torch` package (which is part of the `requirements.txt` for this workshop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "clip_model = foz.load_zoo_model(\n",
    "    \"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"openai/clip-vit-base-patch32\", \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    use_fast=True,\n",
    "    # install_requirements=True # uncomment this line if you are running this code for the first time\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify model architectures and pretrained weights by passing in optional parameters. Pretrained models can be loaded directly from OpenCLIP with the following syntax:\n",
    "\n",
    "\n",
    "```python\n",
    "meta_clip = foz.load_zoo_model(\n",
    "    name_or_url=\"open-clip-torch\",\n",
    "    clip_model=\"ViT-B-32-quickgelu\",\n",
    "    pretrained=\"metaclip_400m\",\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "Alternatively you can also load a model from Hugging Face‚Äôs Model Hub with the following syntax:\n",
    "\n",
    "\n",
    "```python\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "open_clip_model = foz.load_zoo_model(\n",
    "    name_or_url=\"open-clip-torch\",\n",
    "    clip_model=\"hf-hub:repo-name/model-name\",\n",
    "    pretrained=\"\",\n",
    ")\n",
    "```\n",
    "\n",
    "As a concrete example, if you were interested in the [StreetCLIP model](https://huggingface.co/geolocal/StreetCLIP) you would use:\n",
    "\n",
    "```python\n",
    "street_clip_model = foz.load_zoo_model(\n",
    "    name_or_url=\"open-clip-torch\",\n",
    "    pretrained=\"\",\n",
    "    clip_model=\"hf-hub:geolocal/StreetCLIP\"\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hugging Face Integration\n",
    "\n",
    "\n",
    "You can also run models from Hugging Face as a Zoo Model with [FiftyOne's Hugging Face Integration](https://beta-docs.voxel51.com/integrations/huggingface/#zero-shot-classification). \n",
    "\n",
    "To load a model from the Hugging Face Hub, set `name_or_url=zero-shot-classification-transformer-torch`. This specifies that you want to a zero-shot image classification model from the Hugging Face Transformers library. You can then specify the model via the `name_or_path` argument. This should be the repository name or model identifier of the model you want to load:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "siglip_model = foz.load_zoo_model(\n",
    "    \"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"google/siglip2-so400m-patch14-384\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    use_fast=True,\n",
    "    # install_requirements=True # uncomment this line if you are running this code for the first time\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing embeddings\n",
    "\n",
    "We can use [the `compute_embeddings`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#compute_embeddings) method of the Dataset as follows to compute embeddings for the images in the Dataset.\n",
    "\n",
    "This method supports all the following cases:\n",
    "\n",
    "‚Ä¢ Using an image model to compute image embeddings for an image collection\n",
    "\n",
    "‚Ä¢ Using an image model to compute frame embeddings for a video collection\n",
    "\n",
    "‚Ä¢ Using a video model to compute embeddings for a video collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=clip_model,\n",
    "    embeddings_field=\"clip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=siglip_model,\n",
    "    embeddings_field=\"siglip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Visualization\n",
    "\n",
    "Now, we can use UMAP to reduce the dimensionality of the embeddings and explore them in the FiftyOne app. \n",
    "\n",
    "> Note that you will need to have the `umap-learn` package installed for this, which is also listed as a requirement in the `requirements.txt` file of this repository.\n",
    "\n",
    "We can use the [FiftyOne Brain](https://beta-docs.voxel51.com/fiftyone_concepts/brain/) to perform [dimensionality reduction](https://beta-docs.voxel51.com/tutorials/dimension_reduction/) so that we can viualize the embeddings in the FiftyOne App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "embedding_fields = [\n",
    "    \"clip_embeddings\",\n",
    "    \"siglip_embeddings\"\n",
    "]\n",
    "\n",
    "# Compute UMAP for each embedding\n",
    "\n",
    "for field in embedding_fields:\n",
    "    _fname = field.split(\"_embeddings\")[0]\n",
    "    brain_key = f\"{_fname}_viz\"\n",
    "    \n",
    "    results = fob.compute_visualization(\n",
    "        dataset,\n",
    "        embeddings=field,\n",
    "        method=\"umap\",\n",
    "        brain_key=brain_key,\n",
    "        num_dims=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Embeddings\n",
    "\n",
    "You can launch the app in a notebook by running:\n",
    "\n",
    "```python\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "fo.launch_app(hub_dataset)\n",
    "```\n",
    "\n",
    "Or, you can open your terminal and execute `fiftyone app launch`. This will open the App in a browser window. The you will select your Dataset from the dropdown menu, open the embeddings panel by clicking the `+`  next to the Samples viewer, and select the embeddings you want to display by selecting from the dropdown menu in the embeddings panel.\n",
    "\n",
    "## Patch embeddings\n",
    "\n",
    "Note that these embeddings are computed for the entire image. You may find it interesting and useful to compute embeddings for each *patch* of an image. That is, each segmentation mask in the Dataset.\n",
    "\n",
    "You can use the more explicit pattern defined above where we we:\n",
    "\n",
    "- instantiated a model\n",
    "\n",
    "- called the `compute_embeddings` method of the Dataset with that model\n",
    "\n",
    "- called FiftyOne Brain's `compute_visualization` method with those embeddings\n",
    "\n",
    "However, you can directly compute and visualize embeddings with any Zoo Model using the following pattern:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    model=\"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"google/siglip2-so400m-patch14-384\",    \n",
    "    patches_field=\"ground_truth\",\n",
    "    brain_key=\"siglip_viz_patches\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize text embeddings\n",
    "\n",
    "You can also compute embeddings for the `damage_report` field we generated using the VLM and visualize those in the App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import fiftyone.brain as fob\n",
    "from transformers import AutoModel\n",
    "\n",
    "#set an environment variable so tokenizers doesn't yell at us,\n",
    "# note this related to the `transformers` and `tokenizers` libraries and not a FiftyOne specific environment variable\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "jina_embeddings_model = AutoModel.from_pretrained(\n",
    "    \"jinaai/jina-embeddings-v3\", \n",
    "    trust_remote_code=True,\n",
    "    device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "for sample in dataset.iter_samples(autosave=True):\n",
    "    text_embeddings = jina_embeddings_model.encode(\n",
    "        sentences = [sample[\"damage_report\"]], # model expects a list of strings\n",
    "        task=\"separation\"\n",
    "        )\n",
    "    sample[\"text_embeddings\"] = text_embeddings.squeeze()\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "        dataset,\n",
    "        embeddings=\"text_embeddings\",\n",
    "        method=\"umap\",\n",
    "        brain_key=f\"text_embeddings_viz\",\n",
    "        num_dims=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, at this point it will be helpful for us to install the [Caption View Plugin](https://github.com/mythrandire/caption-viewer) so that we can more easily read the captions as we explore them in embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search your images with natural language\n",
    "\n",
    "When you create a similarity index powered by the CLIP model, you can also search by arbitrary natural language queries natively in the App.\n",
    "\n",
    "Note: For the models we used to compute emebeddings above, FiftyOne's implementation uses those model to extract image embeddings. So those models don't *currently* support text prompts, hence we use a model whose implementatin in FiftyOne does support both text and images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    brain_key=\"text_img_sim\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will allow you to search images \"holistically\", that is considering the entire content of an image.\n",
    "\n",
    "However, you can also do the create an index on the *patch* level and search that via natural language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "patch_text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    patches_field=\"ground_truth\",\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    brain_key=\"patch_text_img_sim\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick note on managing Brain runs\n",
    "\n",
    "You can keep track of the Brain runs you have by calling: `dataset.list_brain_runs()`\n",
    "\n",
    "If you were to rerun the cell above again, you'd see the error following error: `ValueError: Brain method run with key 'patch_text_img_sim' already exists`\n",
    "\n",
    "\n",
    "If you want to ever delete a specific Brain run, for example you really like the name `patch_text_img_sim` and want to use it again,  then you can run: `dataset.delete_brain_run(\"patch_text_img_sim\")`\n",
    "\n",
    "‚ö†Ô∏è To delete ALL the Brain runs on a Dataset, you can call (though it is not recommended): `dataset.delete_brain_runs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn to the app and perform some natural language search. Click on the `üîé` icon in the Samples viewer and start searching!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other embeddings based workflows\n",
    "\n",
    "### Computing Representativeness\n",
    "\n",
    "You can use the [`compute_representativeness` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_representativeness) from FiftyOne Brain to compute representativeness \n",
    "\n",
    "**What is Representativeness?**\n",
    "\n",
    "A measure that identifies how well a sample represents typical patterns in your dataset, scored from 0 to 1 (1 being most representative).\n",
    "\n",
    "**Key Uses:**\n",
    "1. Find outliers (low scores)\n",
    "2. Identify typical examples (high scores)\n",
    "3. Guide data augmentation\n",
    "4. Evaluate model performance on typical vs. atypical cases\n",
    "5. Prioritize diverse data labeling\n",
    "\n",
    "**How it Works:**\n",
    "- Uses clustering to group similar images\n",
    "- Scores samples based on proximity to cluster centers\n",
    "- Two methods available:\n",
    "  - `cluster-center`: Favors samples close to cluster centers\n",
    "  - `cluster-center-downweight`: Promotes more diversity\n",
    "\n",
    "**Implementation:**\n",
    "Simply call `compute_representativeness()` on your dataset - no pre-trained model predictions needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "fob.compute_representativeness(\n",
    "    dataset,\n",
    "    embeddings=\"siglip_embeddings\",\n",
    "    representativeness_field=\"siglip_representativeness\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this method for any Field on your Dataset which contains embeddings. Let's also do this for `text_embeddings`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "fob.compute_representativeness(\n",
    "    dataset,\n",
    "    embeddings=\"text_embeddings\", # you can also use \"clip_embeddings\"\n",
    "    representativeness_field=\"text_representativeness\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness\n",
    "\n",
    "You can use [the `compute_uniqueness` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_uniqueness) from FiftyOne Brain to help you quantifiy how distinct or exceptional each sample is compared to others in a dataset, revealing outliers and unusual cases that might otherwise remain hidden in the data.\n",
    "\n",
    "The algorithm identifies which samples are \"outliers\" or unusual compared to the rest of the dataset. A sample is considered unique when it's far from other samples in the feature space. The more isolated a sample is in the feature space (greater distances to neighbors), the higher its uniqueness score will be. This differs from \"representativeness\" which would emphasize samples central to clusters.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Generate Embeddings**: \n",
    "   - Each sample is embedded into a vector space using either:\n",
    "     - A machine learning model (defaults to \"simple-resnet-cifar10\")\n",
    "     - Pre-computed embeddings \n",
    "     - A similarity index\n",
    "\n",
    "2. **Find Nearest Neighbors**:\n",
    "   - For each sample, find its K nearest neighbors (K=3)\n",
    "   - Calculate distances to these neighbors\n",
    "\n",
    "3. **Score Uniqueness**:\n",
    "   - Compute a weighted average of distances to nearest neighbors\n",
    "   - Weights [0.6, 0.3, 0.1] give more importance to the closest neighbor\n",
    "   - Normalize scores by dividing by the maximum (resulting in 0-1 range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "import os \n",
    "\n",
    "fob.compute_uniqueness(\n",
    "    dataset,\n",
    "    embeddings=\"siglip_embeddings\",\n",
    "    uniqueness_field = \"siglip_uniqueness\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
