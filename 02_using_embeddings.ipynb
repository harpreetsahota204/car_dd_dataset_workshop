{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/car_dd_dataset_workshop/blob/main/02_using_embeddings.ipynb)\n",
    "\n",
    "Note: If using in Google Colab, make sure you [install all the requirements listed here](https://github.com/harpreetsahota204/car_dd_dataset_workshop/blob/main/requirements.txt).\n",
    "\n",
    "# Using Embeddings for Deeper Dataset Understanding\n",
    "\n",
    "You can use embeddings to gain a deeper understanding of the images in this dataset.\n",
    "Visual embeddings can help analyze car damage images in several key ways:\n",
    "\n",
    "1. [**Relationship Visualization:**](https://docs.voxel51.com/brain.html#brain-embeddings-visualization) Using dimensionality reduction (like UMAP) to visualize how different damage types cluster together and identify patterns.\n",
    "\n",
    "2. **Model Comparison:** Compare how different vision models encode and interpret car damage, revealing their unique perspectives and strengths.\n",
    "\n",
    "3. **Category Analysis:** Explore visual similarities and differences between the six damage types (dent, scratch, crack, glass shatter, tire flat, lamp broken).\n",
    "\n",
    "4. **Variation Study:** Understand how factors like shooting angle and vehicle color affect damage representation in embedding space.\n",
    "\n",
    "5. **Feature Detection:** Identify subtle visual features that distinguish different types of damage, which might not be obvious in the annotations.\n",
    "\n",
    "For this analysis, we’ll use these models:\n",
    "\n",
    "• CLIP\n",
    "\n",
    "• AIMv2\n",
    "\n",
    "• C-RADIOv3\n",
    "\n",
    "Note that both of these models can be used for zero-shot classification. \n",
    "\n",
    "We won't discuss their use for that task here, but you're encouraged to [learn more about zero-shot classification in this tutorial](https://github.com/harpreetsahota204/getting-started-fo-experiences/blob/main/zero-shot-prediction/zero-shot-classification.ipynb).\n",
    "\n",
    "Start by loading the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset = fo.load_dataset(\"cardd_from_hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # or if you are in a new notebook\n",
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"harpreetsahota/CarDD\",\n",
    "    name=\"cardd_from_hub\",\n",
    "    # max_samples=500, # if you want to work with a subset of the dataset\n",
    "    persistent=True,\n",
    "    overwrite=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a quick function to help you select the best device for your machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the appropriate device for model inference.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "DEVICE = get_device()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you can instantiate the models and then computing embeddings.\n",
    "\n",
    "# Open CLIP Integration\n",
    "\n",
    "FiftyOne [integrates natively with the OpenCLIP library](https://beta-docs.voxel51.com/integrations/openclip/), an open source implementation of OpenAI’s CLIP (Contrastive Language-Image Pre-training) model that you can use to run inference on your FiftyOne datasets with a few lines of code!\n",
    "\n",
    "To use models from OpenCLIP you need to ensure you have installed the `open_clip_torch` package (which is part of the `requirements.txt` for this workshop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "# need to set classes for the model\n",
    "\n",
    "classes = dataset.distinct(\"detections.detections.label\")\n",
    "\n",
    "clip_model = foz.load_zoo_model(\n",
    "    \"open-clip-torch\",\n",
    "    name_or_path=\"openai/clip-vit-base-patch32\", \n",
    "    device=DEVICE,\n",
    "    use_fast=True,\n",
    "    # install_requirements=True # uncomment this line if you are running this code for the first time\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify model architectures and pretrained weights by passing in optional parameters. Pretrained models can be loaded directly from OpenCLIP with the following syntax:\n",
    "\n",
    "\n",
    "```python\n",
    "meta_clip = foz.load_zoo_model(\n",
    "    name_or_url=\"open-clip-torch\",\n",
    "    clip_model=\"ViT-B-32-quickgelu\",\n",
    "    pretrained=\"metaclip_400m\",\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "Alternatively you can also load a model from Hugging Face’s Model Hub with the following syntax:\n",
    "\n",
    "\n",
    "```python\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "open_clip_model = foz.load_zoo_model(\n",
    "    name_or_url=\"open-clip-torch\",\n",
    "    clip_model=\"hf-hub:repo-name/model-name\",\n",
    "    pretrained=\"\",\n",
    ")\n",
    "```\n",
    "\n",
    "As a concrete example, if you were interested in the [StreetCLIP model](https://huggingface.co/geolocal/StreetCLIP) you would use:\n",
    "\n",
    "```python\n",
    "street_clip_model = foz.load_zoo_model(\n",
    "    name_or_url=\"open-clip-torch\",\n",
    "    pretrained=\"\",\n",
    "    clip_model=\"hf-hub:geolocal/StreetCLIP\",\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hugging Face Integration\n",
    "\n",
    "\n",
    "You can also run models from Hugging Face as a Zoo Model with [FiftyOne's Hugging Face Integration](https://beta-docs.voxel51.com/integrations/huggingface/#zero-shot-classification). \n",
    "\n",
    "To load a model from the Hugging Face Hub, set `name_or_url=zero-shot-classification-transformer-torch`. This specifies that you want to a zero-shot image classification model from the Hugging Face Transformers library. You can then specify the model via the `name_or_path` argument. This should be the repository name or model identifier of the model you want to load:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModel\n",
    "import fiftyone.utils.transformers as fout\n",
    "\n",
    "aim_model = AutoModel.from_pretrained(\n",
    "    \"apple/aimv2-large-patch14-448\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following isn't necessary for all models, but if you get an error that says `Unrecognized model` when trying to compute embeddings then a it's usually a sign you need to convert it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aim_fo_model = fout.convert_transformers_model(\n",
    "    aim_model, \n",
    "    trust_remote_code=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing embeddings\n",
    "\n",
    "We can use [the `compute_embeddings`](https://beta-docs.voxel51.com/api/fiftyone.core.collections.SampleCollection.html#compute_embeddings) method of the Dataset as follows to compute embeddings for the images in the Dataset.\n",
    "\n",
    "This method supports all the following cases:\n",
    "\n",
    "• Using an image model to compute image embeddings for an image collection\n",
    "\n",
    "• Using an image model to compute frame embeddings for a video collection\n",
    "\n",
    "• Using a video model to compute embeddings for a video collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=clip_model,\n",
    "    embeddings_field=\"clip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=aim_fo_model,\n",
    "    embeddings_field=\"aim_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a remote source zoo model\n",
    "\n",
    "You can implement any embeddings model into FiftyOne, including your own custom model, and use it as a remote source zoo model.\n",
    "\n",
    "Here we will use [NVIDIA's C-RADIOv3](https://github.com/harpreetsahota204/NVLabs_CRADIOV3/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/NVLabs_CRADIOV3\",\n",
    ")\n",
    "\n",
    "radio_embeddings_model = foz.load_zoo_model(\n",
    "    \"nv_labs/c-radio_v3-b\",\n",
    "    feature_format=\"NCHW\", # you can also pass NLC here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=radio_embeddings_model,\n",
    "    embeddings_field=\"radio_embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cool feature of the C-RADIOv3 model is you can also compute spatial features. \n",
    "\n",
    "To use this feature you need to set `output_type=\"spatial\"`, additionally spatial features only supports `feature_format=\"NCHW\"`.\n",
    "\n",
    "You can choose to do some Gaussian smoothing if you'd like, just set `apply_smoothing=True` and choose a value for `smoothing_sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_spatial_model = foz.load_zoo_model(\n",
    "    \"nv_labs/c-radio_v3-b\",\n",
    "    output_type=\"spatial\",\n",
    "    apply_smoothing=True, # if you want smoothing\n",
    "    smoothing_sigma=0.51, # how much smoothing you want to apply\n",
    "    feature_format=\"NCHW\" #this is the required for the heatmap\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we will need to use the [`apply_model`](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.apply_model) method of the Dataset so we can parse the output as a [Heatmap](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.apply_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(\n",
    "    radio_spatial_model,\n",
    "    \"radio_spatial_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch embeddings\n",
    "\n",
    "Note that these embeddings are computed for the entire image. You may find it interesting and useful to compute embeddings for each *patch* of an image. That is, each bounding box or segmentation mask in the Dataset.\n",
    "\n",
    "You can do this via [`compute_patch_embeddings`](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.compute_patch_embeddings): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_patch_embeddings(\n",
    "    model=radio_embeddings_model,\n",
    "    patches_field=\"detections\",\n",
    "    embeddings_field=\"radio_box_patch_emb\"\n",
    ")\n",
    "\n",
    "dataset.compute_patch_embeddings(\n",
    "    model=radio_embeddings_model,\n",
    "    patches_field=\"segmentations\",\n",
    "    embeddings_field=\"radio_mask_patch_emb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text embeddings\n",
    "\n",
    "You can also compute embeddings for the `damage_report` field we generated using the VLM and visualize those in the App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import fiftyone.brain as fob\n",
    "from transformers import AutoModel\n",
    "\n",
    "#set an environment variable so tokenizers doesn't yell at us,\n",
    "# note this related to the `transformers` and `tokenizers` libraries and not a FiftyOne specific environment variable\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "jina_embeddings_model = AutoModel.from_pretrained(\n",
    "    \"jinaai/jina-embeddings-v3\", \n",
    "    trust_remote_code=True,\n",
    "    device_map = DEVICE\n",
    "    )\n",
    "\n",
    "for sample in dataset.iter_samples(autosave=True):\n",
    "    text_embeddings = jina_embeddings_model.encode(\n",
    "        sentences = [sample[\"damage_report\"]], # model expects a list of strings\n",
    "        task=\"separation\"\n",
    "        )\n",
    "    sample[\"text_embeddings\"] = text_embeddings.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Visualization\n",
    "\n",
    "Now, we can use UMAP to reduce the dimensionality of the embeddings and explore them in the FiftyOne app. \n",
    "\n",
    "> Note that you will need to have the `umap-learn` package installed for this, which is also listed as a requirement in the `requirements.txt` file of this repository.\n",
    "\n",
    "We can use the [FiftyOne Brain](https://beta-docs.voxel51.com/fiftyone_concepts/brain/) to perform [dimensionality reduction](https://beta-docs.voxel51.com/tutorials/dimension_reduction/) so that we can viualize the embeddings in the FiftyOne App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "embedding_fields = [\n",
    "    \"clip_embeddings\",\n",
    "    \"aim_embeddings\",\n",
    "    \"radio_embeddings\",\n",
    "    \"radio_box_patch_emb\",\n",
    "    \"radio_mask_patch_emb\",\n",
    "    \"text_embeddings\"\n",
    "    \n",
    "]\n",
    "\n",
    "# Compute UMAP for each embedding\n",
    "\n",
    "for field in embedding_fields:\n",
    "    _fname = field.split(\"_embeddings\")[0]\n",
    "    brain_key = f\"{_fname}_viz\"\n",
    "    \n",
    "    results = fob.compute_visualization(\n",
    "        dataset,\n",
    "        embeddings=field,\n",
    "        method=\"umap\",\n",
    "        brain_key=brain_key,\n",
    "        num_dims=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Embeddings\n",
    "\n",
    "You can launch the app in a notebook by running:\n",
    "\n",
    "```python\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "fo.launch_app(hub_dataset)\n",
    "```\n",
    "\n",
    "Or, you can open your terminal and execute `fiftyone app launch`. This will open the App in a browser window. The you will select your Dataset from the dropdown menu, open the embeddings panel by clicking the `+`  next to the Samples viewer, and select the embeddings you want to display by selecting from the dropdown menu in the embeddings panel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search your images with natural language\n",
    "\n",
    "When you create a similarity index powered by the CLIP model, you can also search by arbitrary natural language queries natively in the App.\n",
    "\n",
    "Note: For the models we used to compute emebeddings above, FiftyOne's implementation uses those model to extract image embeddings. So those models don't *currently* support text prompts, hence we use a model whose implementatin in FiftyOne does support both text and images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset = fo.load_dataset(\"cardd_from_hub\")\n",
    "\n",
    "dataset.delete_brain_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    brain_key=\"text_img_sim\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can always implement a custom model to perform image-text similarity search. For example here's [the SigLip2 model](https://github.com/harpreetsahota204/siglip2) which was implemented as a remote zoo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Register this custom model source\n",
    "foz.register_zoo_model_source(\"https://github.com/harpreetsahota204/siglip2\")\n",
    "\n",
    "# Download your preferred SigLIP2 variant\n",
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/siglip2\",\n",
    "    model_name=\"google/siglip2-so400m-patch16-384\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "# Build a similarity index\n",
    "text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model=\"google/siglip2-so400m-patch16-384\",\n",
    "    brain_key=\"siglip_img_txt_sim\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick note on managing Brain runs\n",
    "\n",
    "You can keep track of the Brain runs you have by calling: `dataset.list_brain_runs()`\n",
    "\n",
    "If you were to rerun the cell above again, you'd see the error following error: `ValueError: Brain method run with key 'patch_text_img_sim' already exists`\n",
    "\n",
    "\n",
    "If you want to ever delete a specific Brain run, for example you really like the name `patch_text_img_sim` and want to use it again,  then you can run: `dataset.delete_brain_run(\"patch_text_img_sim\")`\n",
    "\n",
    "⚠️ To delete ALL the Brain runs on a Dataset, you can call (though it is not recommended): `dataset.delete_brain_runs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn to the app and perform some natural language search. Click on the `🔎` icon in the Samples viewer and start searching!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other embeddings based workflows\n",
    "\n",
    "### Computing Representativeness\n",
    "\n",
    "You can use the [`compute_representativeness` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_representativeness) from FiftyOne Brain to compute representativeness \n",
    "\n",
    "**What is Representativeness?**\n",
    "\n",
    "A measure that identifies how well a sample represents typical patterns in your dataset, scored from 0 to 1 (1 being most representative).\n",
    "\n",
    "**Key Uses:**\n",
    "1. Find outliers (low scores)\n",
    "2. Identify typical examples (high scores)\n",
    "3. Guide data augmentation\n",
    "4. Evaluate model performance on typical vs. atypical cases\n",
    "5. Prioritize diverse data labeling\n",
    "\n",
    "**How it Works:**\n",
    "- Uses clustering to group similar images\n",
    "- Scores samples based on proximity to cluster centers\n",
    "- Two methods available:\n",
    "  - `cluster-center`: Favors samples close to cluster centers\n",
    "  - `cluster-center-downweight`: Promotes more diversity\n",
    "\n",
    "**Implementation:**\n",
    "Simply call `compute_representativeness()` on your dataset - no pre-trained model predictions needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "fob.compute_representativeness(\n",
    "    dataset,\n",
    "    embeddings=\"radio_embeddings\",\n",
    "    representativeness_field=\"radio_representativeness\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this method for any Field on your Dataset which contains embeddings. Let's also do this for `text_embeddings`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "fob.compute_representativeness(\n",
    "    dataset,\n",
    "    embeddings=\"text_embeddings\", # you can also use \"clip_embeddings\"\n",
    "    representativeness_field=\"text_representativeness\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness\n",
    "\n",
    "You can use [the `compute_uniqueness` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_uniqueness) from FiftyOne Brain to help you quantifiy how distinct or exceptional each sample is compared to others in a dataset, revealing outliers and unusual cases that might otherwise remain hidden in the data.\n",
    "\n",
    "The algorithm identifies which samples are \"outliers\" or unusual compared to the rest of the dataset. A sample is considered unique when it's far from other samples in the feature space. The more isolated a sample is in the feature space (greater distances to neighbors), the higher its uniqueness score will be. This differs from \"representativeness\" which would emphasize samples central to clusters.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Generate Embeddings**: \n",
    "   - Each sample is embedded into a vector space using either:\n",
    "     - A machine learning model (defaults to \"simple-resnet-cifar10\" if you don't pass an embeddings field)\n",
    "     - Pre-computed embeddings \n",
    "     - A similarity index\n",
    "\n",
    "2. **Find Nearest Neighbors**:\n",
    "   - For each sample, find its K nearest neighbors (K=3)\n",
    "   - Calculate distances to these neighbors\n",
    "\n",
    "3. **Score Uniqueness**:\n",
    "   - Compute a weighted average of distances to nearest neighbors\n",
    "   - Weights [0.6, 0.3, 0.1] give more importance to the closest neighbor\n",
    "   - Normalize scores by dividing by the maximum (resulting in 0-1 range)\n",
    "\n",
    "Implementation: Simply call `compute_uniqueness()` on your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "import os \n",
    "\n",
    "fob.compute_uniqueness(\n",
    "    dataset,\n",
    "    embeddings=\"radio_embeddings\",\n",
    "    uniqueness_field = \"radio_uniqueness\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
